{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive transdimensional sampling\n",
    "\n",
    "This notebook implements a naive transdimensional sampler that is based on a sequence of fixed-dimensional Metropolis-Hastings samplers. The goal is to estimate the coefficients of a polynomial of unknown degree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import some Python packages\n",
    "\n",
    "We begin by importing some Python packages for random numbers and for plotting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some Python packages.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set some parameters to make plots nicer.\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams.update({'font.size': 20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Artificial data\n",
    "\n",
    "We will solve a synthetic inverse problem where we try to find the coefficients of a polynomial. For this, we compute artificial data using a polynomial of some degree $N_m-1$, where $N_m$ is the dimension of the model space. \n",
    "\n",
    "In a first step, we define (also for later convenience) a forward model function. In the second step, this is used to actually compute some artificial data that are polluted by normally distributed random errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Forward problem function. -------------------------------------------------\n",
    "\n",
    "def forward(m, x, Nm):\n",
    "    \"\"\"\n",
    "    Definition of the forward problem, which is a polynomial of degree Nm-1.\n",
    "\n",
    "       y= m[0] + m[1]*x + m[2]*x**2.0 + ... + m[Nm-1]*x**(Nm-1) .\n",
    "\n",
    "    :param m: Coefficients of the polynomial. Numpy array of dimension Nm.\n",
    "    :param x: Scalar argument of the polynomial.\n",
    "    :param Nm: Model space dimension.\n",
    "    :return: Value of the polynomial at x.\n",
    "    \"\"\"\n",
    "\n",
    "    d = 0.0\n",
    "\n",
    "    for k in range(Nm):\n",
    "        d+=m[k]*(x**(k))\n",
    "\n",
    "    return d\n",
    "\n",
    "\n",
    "# Input parameters for computation of artificial data. ----------------------\n",
    "\n",
    "# Measurement locations.\n",
    "x = np.arange(0.0,11.0,1.0)\n",
    "\n",
    "# Model parameters and model space dimension.\n",
    "m = np.array([1.0,1.0])\n",
    "Nm = len(m)\n",
    "\n",
    "# Standard deviation of the Gaussian errors.\n",
    "sigma = 2.0\n",
    "\n",
    "# Fixed random seed to make examples reproducible.\n",
    "np.random.seed(3)\n",
    "\n",
    "# Compute artificial data. --------------------------------------------------\n",
    "d = forward(m, x, Nm) + sigma*np.random.randn(len(x))\n",
    "\n",
    "# Plot data. ----------------------------------------------------------------\n",
    "\n",
    "# Plot with errorbars.\n",
    "plt.plot(x, d, 'ko')\n",
    "plt.errorbar(x, d, yerr=sigma, xerr=0.0, ecolor='k', ls='none')\n",
    "\n",
    "# Superimpose regression polynomials up to some degree.\n",
    "for n in range(5):\n",
    "    z = np.polyfit(x, d, n)\n",
    "    p = np.poly1d(z)\n",
    "    d_fit = p(x)\n",
    "    plt.plot(x, d_fit)\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('d')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fixed-dimensional Metropolis-Hastings sampling\n",
    "\n",
    "The naive transdimensional sampler is based on fixed-dimensional sampling. This is done most conveniently (though probably not most efficiently) using global Metropolis-Hastings sampling with the fixed-dimensional prior as proposal distribution.\n",
    "\n",
    "In our toy problem, the priors for all model parameters, regardless of dimension, are uniform. The fixed-dimensional likelihood function is Gaussian.\n",
    "\n",
    "The output of the fixed-dimensional Metropolis-Hastings sampler is a collection of samples and the evidence (approximated by the samples). Some care must be taken here with the size of the problem because all samples are kept in memory!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fixed-dimensional Metropolis-Hastings sampler. ----------------------------\n",
    "\n",
    "def mh(x, d, sigma, Nm, Nsamples):\n",
    "    \"\"\"\n",
    "    Run the Metropolis-Hastings algorithm to sample the fixed-dimensional posterior.\n",
    "\n",
    "    All priors on the model parameters are uniform.\n",
    "\n",
    "    :param x: Measurement locations. Needed to evaluate the forward problem.\n",
    "    :param d: Data.\n",
    "    :param sigma: Standard deviation of the data errors.\n",
    "    :param Nm: Model space dimension.\n",
    "    :param Nsamples: Total number of samples in the Metropolis-Hastings sampling.\n",
    "    :return: samples, evidence. Vector containing the samples, plus the evidence.\n",
    "    \"\"\"\n",
    "\n",
    "    #- Initialisation. ------------------------------------------------------\n",
    "\n",
    "    # Allowable range of the model parameters around 0.\n",
    "    m_range = 5.0\n",
    "\n",
    "    # Allocate empty vectors to collect samples.\n",
    "    samples = np.zeros((Nm, Nsamples))\n",
    "\n",
    "    # Compute initial misfit.\n",
    "    m_current = 2.0*m_range*(np.random.rand(Nm)-0.5)\n",
    "    d_current = forward(m_current, x, Nm)\n",
    "    x_current = np.sum(((d - d_current)**2.0)/(2.0*sigma**2.0))\n",
    "\n",
    "    # Assign first sample and start accumulating the evidence.\n",
    "    samples[:, 0] = m_current\n",
    "    evidence = np.exp(-x_current)\n",
    "\n",
    "    # Sampling. -------------------------------------------------------------\n",
    "\n",
    "    for k in range(1,Nsamples):\n",
    "\n",
    "        # Test sample and misfit.\n",
    "        m_test = 2.0 * m_range * (np.random.rand(Nm) - 0.5)\n",
    "        d_test = forward(m_test, x, Nm)\n",
    "        x_test = np.sum(((d - d_test) ** 2.0) / (2.0 * sigma ** 2.0))\n",
    "\n",
    "        # Accumulate evidence.\n",
    "        evidence += np.exp(-x_test)\n",
    "\n",
    "        # Metropolis rule (in logarithmic form, to avoid exponential overflow).\n",
    "        p = np.minimum(0.0, -x_test+x_current)\n",
    "        if p >= np.log(np.random.rand(1)):\n",
    "            m_current = m_test\n",
    "            x_current = x_test\n",
    "\n",
    "        samples[:, k] = m_current\n",
    "\n",
    "    return samples, evidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run the fixed-dimensional sampler\n",
    "\n",
    "We now actually run the fixed-dimensional sampler for all model space dimensions that we consider. Along the way, we compute the fixed-dimensional evidence, $\\rho(\\mathbf{d}|N_m)$. To obtain a reasonable posterior, around 100'000 should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input parameters. ---------------------------------------------------------\n",
    "\n",
    "# Estimated standard deviation of the data errors.\n",
    "sigma=2.0\n",
    "\n",
    "# Maximum allowable dimension of the model space.\n",
    "Nm_max=4\n",
    "\n",
    "# Total number of samples in each fixed-dimensional sampler.\n",
    "Nsamples=100000\n",
    "\n",
    "# Run fixed dimensional sampler. --------------------------------------------\n",
    "\n",
    "# Initialise lists for the samples and for the evidence.\n",
    "s=[]\n",
    "e=[]\n",
    "\n",
    "# Individual, fixed-dimensional Metropolis-Hastings runs.\n",
    "for Nm in range(1, Nm_max+1):\n",
    "    samples, evidence = mh(x, d, sigma, Nm, Nsamples)\n",
    "    s.append(samples)\n",
    "    e.append(evidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fixed-dimensional marginals\n",
    "\n",
    "Using the fixed-dimensional samples, we may plot fixed-dimensional posterior marginals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot some fixed-dimensional marginals. ------------------------------------\n",
    "\n",
    "# Fixed model space dimension to plot.\n",
    "dimension=2\n",
    "# Model parameter index to plot. (Between 0 and dimension-1.)\n",
    "parameter=1\n",
    "\n",
    "plt.hist(s[dimension-1][parameter,:], bins=15, color='k')\n",
    "plt.xlabel('m'+str(dimension+1))\n",
    "plt.ylabel('posterior marginal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Posterior distribution of model space dimension\n",
    "\n",
    "Using Bayes' theorem for model space dimension, $\\rho(N_m | \\mathbf{d}) = \\rho(\\mathbf{d}|N_m) / \\rho(\\mathbf{d})$, we can compute the posterior for model space dimension, $\\rho(N_m | \\mathbf{d})$. (Here we already assumed that the prior on model space dimension, $\\rho(N_m)$, is uniform.) For this, we first compute the total evidence $\\rho(\\mathbf{d})$ from the normalisation condition, $\\rho(\\mathbf{d}) = \\sum_{N_m} \\rho(\\mathbf{d}|N_m)$. Finally, we simply divide the conditional evidence $\\rho(\\mathbf{d}|N_m)$ by the total evidence $\\rho(\\mathbf{d})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Total (unnormalised) evidence.\n",
    "e_total = sum(e)\n",
    "e = e/e_total\n",
    "\n",
    "# Plot marginal for dimension.\n",
    "x = np.arange(1, Nm_max+1)\n",
    "plt.bar(x, e, align='center', color='k')\n",
    "plt.yscale('log', nonposy='clip')\n",
    "plt.xlabel('dimension')\n",
    "plt.ylabel('posterior')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
