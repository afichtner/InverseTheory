{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMC in ND with LF-BFGS autotuning of the mass matrix\n",
    "\n",
    "This notebook implements an autotuning HMC for an N-dimensional distribution based on limited-memory, factorised BFGS (LF-BFGS) updating of the inverse Hessian.\n",
    "\n",
    "Compared to the BFGS and F-BFGS variants of autotuning, the method implemented here is considerably more complex for two reasons: (1) The initial matrix factor $\\mathbf{S}_0$ can be updated. While this help to effectively produce longer-term memory, it also can lead to a poorly conditioned Hessian, and therefore to leap-frog instability. (2) The leap-frog integration time step is adaptive and depends on the average acceptance rate. Also this may lead to numerical instability if not done conservatively.\n",
    "\n",
    "It is therefore advisable to not have the updating of $\\mathbf{S}_0$ and the adaptive time stepping working at the same time. One possible strategy is to first do a few updates of $\\mathbf{S}_0$ during the first 'few' samples, and then start with the adaptive time stepping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "\n",
    "import testfunctions\n",
    "import samplestatistics\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Times\"\n",
    "plt.rcParams.update({'font.size': 50})\n",
    "plt.rcParams['xtick.major.pad']='12'\n",
    "plt.rcParams['ytick.major.pad']='12'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Input\n",
    "\n",
    "We first define several input parameters, including the model space dimension, the initial inverse mass matrix $\\mathbf{M}^{-1}$, the total number of samples, the number of leapfrog timesteps, and the length of the timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import input_parameters\n",
    "reload(input_parameters)\n",
    "test_function,dim,N,Nit,dt0,m0,Minv,autotune,ell,update_interval,preco,S0_min,plot_interval,dimension1,dimension2,m1_min,m1_max,m2_min,m2_max=input_parameters.input_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Class for LF-BFGS autotuning\n",
    "\n",
    "This class takes care of the limited-memory, factorised BFGS updating. The class must be initialised with the first model and the first gradient. The *update* function then takes the next model and gradient, and computes updates of the approximate matrix factor $\\mathbf{S}$, of the approximate Hessian $\\mathbf{H}$, and of the approximate inverse Hessian $\\mathbf{H}^{-1}$.\n",
    "\n",
    "**Call to caution**: There is an experimental component in this class. In principle, BFGS updates can only be made when $\\mathbf{s}_k^T\\mathbf{y}>0$. However, when this quantity is very small, the resulting Hessian approximation may still be close to singular. Empirically, it is better for stability to choose $\\mathbf{s}_k^T\\mathbf{y}>\\gamma$, with some tuning parameter $\\gamma>0$. For many examples, $\\gamma=2$ works very well. In general, it seems to make sense to check with a little test run, without autotuning, what typical values of $\\mathbf{s}_k^T\\mathbf{y}$ actually are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lfbfgs:\n",
    "    \n",
    "    # Initialisation. ==================================================================\n",
    "    \n",
    "    def __init__(self,dim,Minv,k):\n",
    "        \"\"\"\n",
    "        Initialise the LF-BFGS iteration.\n",
    "        \n",
    "        :param dim: number of model-space dimensions\n",
    "        :param Minv: diagonal elements of the initial inverse mass matrix\n",
    "        :param k: maximum number of vectors to be stored\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dim=dim # Model space dimension.\n",
    "        self.k=k # Maximum number of vectors.\n",
    "        self.i=0 # Currently stored vectors.\n",
    "        \n",
    "        self.s=np.zeros((dim,k)) # s vectors.\n",
    "        self.y=np.zeros((dim,k)) # y vectors.\n",
    "        self.u=np.zeros((dim,k)) # u vectors.\n",
    "        self.v=np.zeros((dim,k)) # v vectors.\n",
    "        self.vTu=np.zeros(k) # Precomputed 1+vT*u.\n",
    "        \n",
    "        self.S0=np.ones(dim) # Diagonal components of the initial S factor.\n",
    "        for i in range(dim):\n",
    "            self.S0[i]=1.0/np.sqrt(Minv[i])\n",
    "            \n",
    "    # Compute and store new vectors u and v. ===========================================\n",
    "        \n",
    "    def put_sy(self,s,y):\n",
    "        \"\"\"\n",
    "        Put in a new pair of vectors s (model difference) and y (gradient difference).\n",
    "        \n",
    "        :param s: current model difference vector\n",
    "        :param y: current gradient difference vector\n",
    "        \"\"\"\n",
    "        \n",
    "        #===============================================================================\n",
    "        # Compute new s, y, u, v vectors.\n",
    "        #===============================================================================\n",
    "        \n",
    "        sy=np.dot(s,y)\n",
    "        #print(sy)\n",
    "        \n",
    "        # Do nothing unless rho is positive.\n",
    "        if sy>10.0 and self.k>0:\n",
    "             \n",
    "            # Precompute inverse Hessian-vector product.\n",
    "            Hinv_y=self.Hinv(y)\n",
    "        \n",
    "            # Auxiliary scalars and vectors.\n",
    "            rho=1.0/sy\n",
    "            gamma2=rho**2 * np.dot(y,Hinv_y) + rho\n",
    "            beta=gamma2 * np.dot(s,self.H(s))\n",
    "            \n",
    "            # Check if certain quantities are actually positive.\n",
    "            if (gamma2>0.0) and (beta>0.0):\n",
    "            \n",
    "                #theta=np.sqrt(rho/(beta*gamma2)) # Here, one may also choose the positive square root. Empirically, the negative ones works much better because it leads to a diagonally dominant S, which is better for preconditioning.\n",
    "                theta=-np.sqrt(rho/(beta*gamma2)) \n",
    "                a=np.sqrt(gamma2)*s\n",
    "                b=(rho/np.sqrt(gamma2))*Hinv_y\n",
    "            \n",
    "                # Compute the current u and v.\n",
    "                u=a\n",
    "                v=-self.H(b+theta*a)\n",
    "         \n",
    "                #===============================================================================\n",
    "                # Update s, y, u, v in the memory.\n",
    "                #===============================================================================\n",
    "        \n",
    "                # When less then k vector pairs are stored, we increase the index of stored pairs and add the new pair.\n",
    "                if self.i<self.k:\n",
    "                    self.i+=1\n",
    "                # Otherwise we kick out the pair with the lowest index by rolling to the left and over-writing the last pair.\n",
    "                else:\n",
    "                    self.s=np.roll(self.s, -1, axis=1)\n",
    "                    self.y=np.roll(self.y, -1, axis=1)\n",
    "                    self.u=np.roll(self.u, -1, axis=1)\n",
    "                    self.v=np.roll(self.v, -1, axis=1)\n",
    "                    self.vTu=np.roll(self.vTu,-1)\n",
    "\n",
    "                self.s[:,self.i-1]=s\n",
    "                self.y[:,self.i-1]=y\n",
    "                self.u[:,self.i-1]=u\n",
    "                self.v[:,self.i-1]=v\n",
    "                self.vTu[self.i-1]=1.0+np.dot(v,u)\n",
    "                \n",
    "            else:\n",
    "                print('LF-BFGS check failed (gamma=%f, beta=%f)' % (gamma2,beta))\n",
    "            \n",
    "        else: \n",
    "            print('LF-BFGS check failed (1/rho=%f)' % sy)\n",
    "\n",
    "    # Implement matrix-vector products. ===============================================\n",
    "    \n",
    "    def S(self,h):\n",
    "        \"\"\"\n",
    "        S*h, product of factor S with some vector h\n",
    "        :param h: vector of dimension self.dim\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(self.i):\n",
    "            h=h-self.v[:,i]*np.dot(self.u[:,i],h)/self.vTu[i]\n",
    "        return h*self.S0\n",
    "    \n",
    "    def Sn(self,h,n):\n",
    "        \"\"\"\n",
    "        S*h, product of factor S with some vector h using a smaller number n of vectors\n",
    "        :param h: vector of dimension self.dim\n",
    "        :param n: number of vectors to take into account\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(n):\n",
    "            h=h-self.v[:,i]*np.dot(self.u[:,i],h)/self.vTu[i]\n",
    "        return h*self.S0\n",
    "    \n",
    "    def ST(self,h):\n",
    "        \"\"\"\n",
    "        S^T*h, product of transposed factor S with some vector h\n",
    "        :param h: vector of dimension self.dim\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(self.i-1,-1,-1):\n",
    "            h=h-self.u[:,i]*np.dot(self.v[:,i],h)/self.vTu[i]\n",
    "        return h*self.S0\n",
    "        \n",
    "    def Sinv(self,h):\n",
    "        \"\"\"\n",
    "        S^-1*h, product of inverse factor S with some vector h\n",
    "        :param h: vector of dimension self.dim\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(self.i-1,-1,-1):\n",
    "            h=h+self.v[:,i]*np.dot(self.u[:,i],h)\n",
    "        return h/self.S0\n",
    "    \n",
    "    def SinvT(self,h):\n",
    "        \"\"\"\n",
    "        S^-T*h, product of inverse transposed factor S with some vector h\n",
    "        :param h: vector of dimension self.dim\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(self.i):\n",
    "            h=h+self.u[:,i]*np.dot(self.v[:,i],h)\n",
    "        return h/self.S0\n",
    "    \n",
    "    def H(self,h):\n",
    "        \"\"\"\n",
    "        H*h, product of Hessian approximation H with some vector h\n",
    "        :param h: vector of dimension self.dim\n",
    "        \"\"\"\n",
    "        \n",
    "        h=self.ST(h)\n",
    "        return self.S(h)\n",
    "    \n",
    "    def Hinv(self,h):\n",
    "        \"\"\"\n",
    "        Hinv*h, product of inverse Hessian approximation H with some vector h\n",
    "        :param h: vector of dimension self.dim\n",
    "        \"\"\"\n",
    "        \n",
    "        h=self.Sinv(h)\n",
    "        return self.SinvT(h)\n",
    "    \n",
    "    # Implement log determinant of S. ==================================================\n",
    "    def logdet(self):\n",
    "        \"\"\"\n",
    "        Current log of the determinant of S\n",
    "        \"\"\"\n",
    "        \n",
    "        logdet=0.0\n",
    "        for i in range(self.i):\n",
    "            alpha=1.0/(1.0+np.dot(self.u[:,i],self.v[:,i]))\n",
    "            logdet+=np.log(alpha**2)\n",
    "            \n",
    "        return logdet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Function for updating the initial matrix factor $\\mathbf{S}_0$\n",
    "\n",
    "The updating of the initial matrix factor $\\mathbf{S}_0$, which may be done repeatedly, can on one hand greatly accelerate convergence. On the other hand, it can be a major source of instability because the resulting Hessian approximations may be poorly conditioned. This updating mechanism distinguishes the limited-memory variant of the autotuning algorithm from its matrix-based variants using (F-)BFGS.\n",
    "\n",
    "The updating implemented here only updates the diagonal of $\\mathbf{S}_0$ using the diagonal of the current Hessian approximation. Empirically it improves convergence when the diagonal is smoothed quite substantially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_S0(M,S0_min):\n",
    "    \"\"\"\n",
    "    Iterative updating of the initial matrix factor S0.\n",
    "    \n",
    "    :param M: an lfbfgs object which needs to be updated.\n",
    "    :param S0_min: minimum allowable diagonal entry of S0. Used for stabilisation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get diagonal elements of the current inverse Hessian.\n",
    "    Minv_new=np.zeros(M.dim)\n",
    "    for i in range(M.dim):\n",
    "        ei=np.zeros(M.dim)\n",
    "        ei[i]=1.0\n",
    "        Hinv_ii=M.Hinv(ei)[i]\n",
    "        # Compute new initial diagonal elements from current ones, in case they are numbers.\n",
    "        if np.isnan(Hinv_ii): Minv_new[i]=1.0/(M.S0[i]**2.0)\n",
    "        else: Minv_new[i]=np.min(( Hinv_ii , 1.0/(S0_min**2.0) ))\n",
    "        \n",
    "    # Smooth diagonal. Empirically improves convergence.\n",
    "    for k in range(100):\n",
    "        Minv_new[0]=(2.0*Minv_new[0]+Minv_new[1])/3.0\n",
    "        Minv_new[-1]=(2.0*Minv_new[-1]+Minv_new[-2])/3.0\n",
    "        Minv_new[1:M.dim-1]=(Minv_new[0:M.dim-2]+Minv_new[1:M.dim-1]+Minv_new[2:M.dim])/3.0\n",
    "       \n",
    "    # Initialise new lfbfgs class member.\n",
    "    M_new=lfbfgs(M.dim,Minv_new,M.k)\n",
    "    \n",
    "    #plt.plot(Minv_new)\n",
    "    #plt.show()\n",
    "    \n",
    "    # Return the new lfbfgs class member.\n",
    "    return M_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Leapfrog integrator\n",
    "\n",
    "For clarity, we define the leap-frog integrator as a separate function. The numerical stability of leapfrog depends on the eigenvalues of the Hessian, and so it may change in the course of the sampling. The implementation below tries to catch potential instabilities by resetting the model and momentum to a random value in case the model values grow beyond resonable values. This may need adjustment in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leapfrog(m,p,Nt,dt,lfbfgs,fct,plot=False):\n",
    "    \"\"\"\n",
    "    Leapfrog time stepping with possible plotting of the trajectory.\n",
    "    \n",
    "    :param m: current model\n",
    "    :param p: current momentum\n",
    "    :param Nt: maximum number of time steps\n",
    "    :param dt: time step lenth\n",
    "    :param lfbfgs: LFBFGS object\n",
    "    :param fct: test function object\n",
    "    :param plot: plot trajectory if True\n",
    "    \"\"\"\n",
    "    \n",
    "    # Plot probability density in the background.\n",
    "    if plot:\n",
    "        fct.plotU(dim,dimension1,dimension2,m1_min,m1_max,m2_min,m2_max)\n",
    "        plt.plot(m[dimension1],m[dimension2],'bo',MarkerSize=15)\n",
    "    \n",
    "    # Evaluate initial gradient.\n",
    "    J=fct.J(m)\n",
    "    \n",
    "    # Determine randomised integration length.\n",
    "    if Nt>=2: Nti=np.int(Nt*(1.0-0.5*np.random.rand()))\n",
    "    else: Nti=Nt\n",
    "    \n",
    "    # Leapfrog integration.\n",
    "    for k in range(Nti):\n",
    "        \n",
    "        # Save initial model for later plotting and to catch instabilities.\n",
    "        m_old=m.copy()\n",
    "        \n",
    "        # Perform one time step.\n",
    "        p=p-0.5*dt*J\n",
    "        m=m+dt*lfbfgs.Hinv(p)\n",
    "        J=fct.J(m)\n",
    "        p=p-0.5*dt*J\n",
    "        \n",
    "        # Catch potential instability.\n",
    "        if np.max(np.abs(m))>100.0:\n",
    "            print('LEAPFROG INSTABILITY')\n",
    "            print(np.max(np.abs(m)))\n",
    "            m=np.random.randn(np.shape(m)[0])\n",
    "            p=np.random.randn(np.shape(m)[0])\n",
    "            return m, p\n",
    "        \n",
    "        # Plot trajectory segment.\n",
    "        if plot: \n",
    "            if k==0: print('number of time steps: %d' % Nti)\n",
    "            plt.plot([m_old[dimension1],m[dimension1]],[m_old[dimension2],m[dimension2]],'r',linewidth=3)\n",
    "            plt.plot(m[dimension1],m[dimension2],'kx',markersize=15)\n",
    "        \n",
    "    return m, p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. HMC initialisations\n",
    "\n",
    "Before running the actual HMC sampler, we perform several initialisations. This includes the test function class, the first random model $\\mathbf{m}$, and the corresponding gradient of the potential energy $\\mathbf{g}=\\nabla U$. With this, we can initialise the BFGS class, which takes $\\mathbf{m}$ and $\\mathbf{g}$ as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation. =============================================================\n",
    "\n",
    "# Test function class.\n",
    "fct=testfunctions.f(dim,test_function)\n",
    "\n",
    "# Initial time step.\n",
    "dt=dt0\n",
    "\n",
    "# Number of accepted models.\n",
    "accept=np.zeros(N)\n",
    "# Current averaged acceptance rate.\n",
    "average_accept=np.ones(N)\n",
    "# Current time step.\n",
    "time_step=dt*np.ones(N)\n",
    "\n",
    "# Initial model (deterministic or randomly chosen).\n",
    "m=m0.copy()\n",
    "\n",
    "# Posterior statistics.\n",
    "stats=samplestatistics.stats(dimension1,dimension2,N)\n",
    "stats.get(m,0.0,0)\n",
    "\n",
    "# Initialise LF-BFGS.\n",
    "g=fct.J(m)\n",
    "M=lfbfgs(dim,Minv.diagonal(),ell)\n",
    "\n",
    "# Specific matrix elements for monitoring.\n",
    "m11=Minv[dimension1,dimension1]*np.ones(N)\n",
    "m22=Minv[dimension2,dimension2]*np.ones(N)\n",
    "\n",
    "# Unit vectors needed to obtain specific mass matrix components for monitoring.\n",
    "e1=np.zeros(dim)\n",
    "e2=np.zeros(dim)\n",
    "e1[dimension1]=1.0\n",
    "e2[dimension2]=1.0\n",
    "\n",
    "# Last iteration when the adaptive time step was changed.\n",
    "it_change_dt=0\n",
    "\n",
    "# Talk or not.\n",
    "verbose=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run HMC\n",
    "\n",
    "We finally run the HMC sampler. In each iteration, we first produce radom momenta $\\mathbf{p}$ from a normal distribution with covariance chosen to be the BFGS-updated inverse mass matrix $\\mathbf{M}^{-1}$, which is defined to be the inverse Hessian $\\mathbf{H}^{-1}$ of the potential energy $U$. \n",
    "\n",
    "Using the mass matrix, we compute energies and run a leapfrog iteration to solve Hamilton's equations. Following this, we compute the energies of the proposed model and evaluate the modified Metropolis rule (in logarithimic form, to avoid over- or under-flow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "\n",
    "# Sampling. ===================================================================\n",
    "\n",
    "for it in range(N-1):\n",
    "\n",
    "    # Randomly choose momentum.\n",
    "    p=np.random.randn(dim)\n",
    "    p=M.S(p)\n",
    "    \n",
    "    # Evaluate energies.\n",
    "    U=fct.U(m)\n",
    "    K=0.5*np.dot(p,M.Hinv(p))\n",
    "    H=U+K\n",
    "    \n",
    "    # Check if models and trajectories should be plotted.\n",
    "    if (not it % plot_interval) and it>0: \n",
    "        plot=True\n",
    "        print('iteration: %d' % it)\n",
    "    else:\n",
    "        plot=False\n",
    "    \n",
    "    # Run leapfrog iteration.\n",
    "    m_new,p_new=leapfrog(m,p,Nit,dt,M,fct,plot)\n",
    "    plt.show()\n",
    "        \n",
    "    # Plot proposed models.\n",
    "    if plot:\n",
    "        plt.subplots(1, figsize=(30,10))\n",
    "        plt.plot(m_new,'k')\n",
    "        plt.xlabel('model parameter index')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        \n",
    "    # Evaluate new energies.\n",
    "    U_new=fct.U(m_new)\n",
    "    K_new=0.5*np.dot(p_new,M.Hinv(p_new))\n",
    "    H_new=U_new+K_new\n",
    "    \n",
    "    # Evaluate Metropolis rule in logarithmic form.\n",
    "    alpha=np.minimum(0.0,H-H_new)\n",
    "    if alpha>=np.log(np.random.rand(1)):\n",
    "        # Accepted.\n",
    "        if verbose: print('it=%d accepted' % it)\n",
    "        accept[it]=1\n",
    "        # Compute s and y vectors.\n",
    "        s=m_new-m\n",
    "        g_new=fct.J(m_new)\n",
    "        y=g_new-g  \n",
    "        # Update model and gradient.\n",
    "        m=m_new\n",
    "        g=g_new\n",
    "        if autotune and (not it % update_interval): M.put_sy(s,y)\n",
    " \n",
    "    # Preconditioning of the S0 diagonal.\n",
    "    if autotune and preco and (not it % 5) and (it>=ell) and (it<=ell+30): M=update_S0(M,S0_min)\n",
    "    #if autotune and preco and (it==ell): M=update_S0(M,S0_min)\n",
    "        \n",
    "    # Accumulate on-the-fly statistics.\n",
    "    m11[it+1]=np.dot(e1,M.Hinv(e1))\n",
    "    m22[it+1]=np.dot(e2,M.Hinv(e2))\n",
    "    stats.get(m,M.logdet(),it+1)\n",
    "    \n",
    "    # Adaptive time stepping.\n",
    "    Navg=20\n",
    "    if autotune and (it>=Navg) and (it-it_change_dt>Navg/2):\n",
    "        average_accept[it]=np.sum(accept[it-Navg+1:it+1])/np.float(Navg)\n",
    "        time_step[it]=dt\n",
    "        if verbose: print('it=%d average acceptance rate: %f' % (it,average_accept[it]))\n",
    "        if average_accept[it]<0.35:\n",
    "            dt=0.8*dt\n",
    "            time_step[it]=dt\n",
    "            print('--> time step reduced to %f in iteration %d' % (dt,it))\n",
    "            it_change_dt=it\n",
    "        elif average_accept[it]>0.95:\n",
    "            dt=1.25*dt\n",
    "            time_step[it]=dt\n",
    "            print('--> time step increased to %f in iteration %d' % (dt,it))\n",
    "            it_change_dt=it\n",
    "\n",
    "# Store final diagonal of the mass matrix.\n",
    "np.save('OUTPUT/S0.npy',M.S0)\n",
    "\n",
    "# Output basic statistics.\n",
    "stop=time.time()\n",
    "print('acceptance rate: %f (%d of %d samples)' % (np.sum(accept)/np.float(N),np.sum(accept),N))\n",
    "print('elapsed time: %f s' % (stop-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyse results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Acceptance and adaptive time stepping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Average acceptance rate.\n",
    "plt.subplots(1, figsize=(20,10))\n",
    "plt.plot(average_accept,'k')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('acceptance rate averaged over %d samples' % Navg)\n",
    "plt.grid()\n",
    "plt.savefig('OUTPUT/acceptance_rate.png', bbox_inches='tight', format='png')\n",
    "plt.show()\n",
    "\n",
    "# Adaptive time step.\n",
    "plt.subplots(1, figsize=(20,10))\n",
    "plt.plot(time_step,'k')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('time step')\n",
    "plt.grid()\n",
    "plt.savefig('OUTPUT/time_step.png', bbox_inches='tight', format='png')\n",
    "plt.show()\n",
    "\n",
    "# Acceptance per sample.\n",
    "plt.subplots(1, figsize=(30,10))\n",
    "plt.plot(accept,'kx')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2. Sample statistics collected on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stats.display()\n",
    "stats.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
