{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMC in ND with LF-BFGS autotuning of the mass matrix\n",
    "\n",
    "This notebook implements an autotuning HMC for an N-dimensional distribution based on limited-memory, factorised BFGS (LF-BFGS) updating of the inverse Hessian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "\n",
    "import testfunctions\n",
    "import samplestatistics\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Times\"\n",
    "plt.rcParams.update({'font.size': 50})\n",
    "plt.rcParams['xtick.major.pad']='12'\n",
    "plt.rcParams['ytick.major.pad']='12'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Input\n",
    "\n",
    "We first define several input parameters, including the model space dimension, the initial inverse mass matrix $\\mathbf{M}^{-1}$, the total number of samples, the number of leapfrog timesteps, and the length of the timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import input_parameters\n",
    "reload(input_parameters)\n",
    "test_function,dim,N,Nit,dt0,m0,Minv,autotune,ell,update_interval,preco,S0_min,plot_interval,dimension1,dimension2=input_parameters.input_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Class for LF-BFGS autotuning\n",
    "\n",
    "This class takes care of the limited-memory, factorised BFGS updating. The class must be initialised with the first model and the first gradient. The *update* function then takes the next model and gradient, and computes updates of the approximate matrix factor $\\mathbf{S}$, of the approximate Hessian $\\mathbf{H}$, and of the approximate inverse Hessian $\\mathbf{H}^{-1}$.\n",
    "\n",
    "Updates are computed only when $\\rho=1/\\mathbf{s}^T\\mathbf{y}>0$. Also, updates are regularised to ensure that the change in the determinant is bounded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lfbfgs:\n",
    "    \n",
    "    # Initialisation. ==================================================================\n",
    "    \n",
    "    def __init__(self,dim,Minv,k):\n",
    "        \"\"\"\n",
    "        Initialise the LF-BFGS iteration.\n",
    "        \n",
    "        :param dim: number of model-space dimensions\n",
    "        :param Minv: diagonal elements of the initial inverse mass matrix\n",
    "        :param k: maximum number of vectors to be stored\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dim=dim # Model space dimension.\n",
    "        self.k=k # Maximum number of vectors.\n",
    "        self.i=0 # Currently stored vectors.\n",
    "        \n",
    "        self.s=np.zeros((dim,k)) # s vectors.\n",
    "        self.y=np.zeros((dim,k)) # y vectors.\n",
    "        self.u=np.zeros((dim,k)) # u vectors.\n",
    "        self.v=np.zeros((dim,k)) # v vectors.\n",
    "        self.vTu=np.zeros(k) # Precomputed 1+vT*u.\n",
    "        \n",
    "        self.S0=np.ones(dim) # Diagonal components of the initial S factor.\n",
    "        for i in range(dim):\n",
    "            self.S0[i]=1.0/np.sqrt(Minv[i])\n",
    "            \n",
    "    # Compute and store new vectors u and v. ===========================================\n",
    "        \n",
    "    def put_sy(self,s,y):\n",
    "        \"\"\"\n",
    "        Put in a new pair of vectors s (model difference) and y (gradient difference).\n",
    "        \n",
    "        :param s: current model difference vector\n",
    "        :param y: current gradient difference vector\n",
    "        \"\"\"\n",
    "        \n",
    "        #===============================================================================\n",
    "        # Compute new s, y, u, v vectors.\n",
    "        #===============================================================================\n",
    "        \n",
    "        sy=np.dot(s,y)\n",
    "        \n",
    "        # Do nothing unless rho is positive.\n",
    "        if sy>0 and self.k>0:\n",
    "             \n",
    "            # Precompute inverse Hessian-vector product.\n",
    "            Hinv_y=self.Hinv(y)\n",
    "        \n",
    "            # Auxiliary scalars and vectors.\n",
    "            rho=1.0/sy\n",
    "            gamma2=rho**2 * np.dot(y,Hinv_y) + rho\n",
    "            beta=gamma2 * np.dot(s,self.H(s))\n",
    "            \n",
    "            # Check if certain quantities are actually positive.\n",
    "            if (gamma2>0.0) and (beta>0.0):\n",
    "            \n",
    "                #theta=np.sqrt(rho/(beta*gamma2)) # Here, one may also choose the positive square root. Empirically, the negative ones works much better because it leads to a diagonally dominant S, which is better for preconditioning.\n",
    "                theta=-np.sqrt(rho/(beta*gamma2)) \n",
    "                a=np.sqrt(gamma2)*s\n",
    "                b=(rho/np.sqrt(gamma2))*Hinv_y\n",
    "            \n",
    "                # Compute the current u and v.\n",
    "                u=a\n",
    "                v=-self.H(b+theta*a)\n",
    "         \n",
    "                #===============================================================================\n",
    "                # Update s, y, u, v in the memory.\n",
    "                #===============================================================================\n",
    "        \n",
    "                # When less then k vector pairs are stored, we increase the index of stored pairs and add the new pair.\n",
    "                if self.i<self.k:\n",
    "                    self.i+=1\n",
    "                # Otherwise we kick out the pair with the lowest index by rolling to the left and over-writing the last pair.\n",
    "                else:\n",
    "                    self.s=np.roll(self.s, -1, axis=1)\n",
    "                    self.y=np.roll(self.y, -1, axis=1)\n",
    "                    self.u=np.roll(self.u, -1, axis=1)\n",
    "                    self.v=np.roll(self.v, -1, axis=1)\n",
    "                    self.vTu=np.roll(self.vTu,-1)\n",
    "\n",
    "                self.s[:,self.i-1]=s\n",
    "                self.y[:,self.i-1]=y\n",
    "                self.u[:,self.i-1]=u\n",
    "                self.v[:,self.i-1]=v\n",
    "                self.vTu[self.i-1]=1.0+np.dot(v,u)\n",
    "                \n",
    "            else:\n",
    "                print('LF-BFGS check failed (gamma=%f, beta=%f)' % (gamma2,beta))\n",
    "            \n",
    "        else: \n",
    "            print('LF-BFGS check failed (1/rho=%f)' % sy)\n",
    "\n",
    "    # Implement matrix-vector products. ===============================================\n",
    "    \n",
    "    def S(self,h):\n",
    "        \"\"\"\n",
    "        S*h, product of factor S with some vector h\n",
    "        :param h: vector of dimension self.dim\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(self.i):\n",
    "            h=h-self.v[:,i]*np.dot(self.u[:,i],h)/self.vTu[i]\n",
    "        return h*self.S0\n",
    "    \n",
    "    def Sn(self,h,n):\n",
    "        \"\"\"\n",
    "        S*h, product of factor S with some vector h using a smaller number n of vectors\n",
    "        :param h: vector of dimension self.dim\n",
    "        :param n: number of vectors to take into account\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(n):\n",
    "            h=h-self.v[:,i]*np.dot(self.u[:,i],h)/self.vTu[i]\n",
    "        return h*self.S0\n",
    "    \n",
    "    def ST(self,h):\n",
    "        \"\"\"\n",
    "        S^T*h, product of transposed factor S with some vector h\n",
    "        :param h: vector of dimension self.dim\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(self.i-1,-1,-1):\n",
    "            h=h-self.u[:,i]*np.dot(self.v[:,i],h)/self.vTu[i]\n",
    "        return h*self.S0\n",
    "        \n",
    "    def Sinv(self,h):\n",
    "        \"\"\"\n",
    "        S^-1*h, product of inverse factor S with some vector h\n",
    "        :param h: vector of dimension self.dim\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(self.i-1,-1,-1):\n",
    "            h=h+self.v[:,i]*np.dot(self.u[:,i],h)\n",
    "        return h/self.S0\n",
    "    \n",
    "    def SinvT(self,h):\n",
    "        \"\"\"\n",
    "        S^-T*h, product of inverse transposed factor S with some vector h\n",
    "        :param h: vector of dimension self.dim\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(self.i):\n",
    "            h=h+self.u[:,i]*np.dot(self.v[:,i],h)\n",
    "        return h/self.S0\n",
    "    \n",
    "    def H(self,h):\n",
    "        \"\"\"\n",
    "        H*h, product of Hessian approximation H with some vector h\n",
    "        :param h: vector of dimension self.dim\n",
    "        \"\"\"\n",
    "        \n",
    "        h=self.ST(h)\n",
    "        return self.S(h)\n",
    "    \n",
    "    def Hinv(self,h):\n",
    "        \"\"\"\n",
    "        Hinv*h, product of inverse Hessian approximation H with some vector h\n",
    "        :param h: vector of dimension self.dim\n",
    "        \"\"\"\n",
    "        \n",
    "        h=self.Sinv(h)\n",
    "        return self.SinvT(h)\n",
    "    \n",
    "    # Implement log determinant of S. ==================================================\n",
    "    def logdet(self):\n",
    "        \"\"\"\n",
    "        Current log of the determinant of S\n",
    "        \"\"\"\n",
    "        \n",
    "        logdet=0.0\n",
    "        for i in range(self.i):\n",
    "            alpha=1.0/(1.0+np.dot(self.u[:,i],self.v[:,i]))\n",
    "            logdet+=np.log(alpha**2)\n",
    "            \n",
    "        return logdet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Function for updating the initial matrix factor $\\mathbf{S}_0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_S0(M,S0_min):\n",
    "    \"\"\"\n",
    "    Iterative updating of the initial matrix factor S0.\n",
    "    \n",
    "    :param M: an lfbfgs object which needs to be updated.\n",
    "    :param S0_min: minimum allowable diagonal entry of S0. Used for stabilisation.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get diagonal elements of the current inverse Hessian.\n",
    "    Minv_new=np.zeros(M.dim)\n",
    "    for i in range(M.dim):\n",
    "        ei=np.zeros(M.dim)\n",
    "        ei[i]=1.0\n",
    "        Hinv_ii=M.Hinv(ei)[i]\n",
    "        # Compute new initial diagonal elements from current ones, in case they are numbers.\n",
    "        if np.isnan(Hinv_ii): Minv_new[i]=1.0/(M.S0[i]**2.0)\n",
    "        else: Minv_new[i]=np.min(( Hinv_ii , 1.0/(S0_min**2.0) ))\n",
    "        \n",
    "    # Smooth diagonal. Empirically improves convergence.\n",
    "    for k in range(10):\n",
    "        Minv_new[0]=(2.0*Minv_new[0]+Minv_new[1])/3.0\n",
    "        Minv_new[-1]=(2.0*Minv_new[-1]+Minv_new[-2])/3.0\n",
    "        Minv_new[1:M.dim-1]=(Minv_new[0:M.dim-2]+Minv_new[1:M.dim-1]+Minv_new[2:M.dim])/3.0\n",
    "       \n",
    "    # Initialise new lfbfgs class member.\n",
    "    M_new=lfbfgs(M.dim,Minv_new,M.k)\n",
    "    \n",
    "    # Assimilate the previous s and y vectors into M_new.\n",
    "    for i in range(M.i): M_new.put_sy(M.s[:,i],M.y[:,i])\n",
    "        \n",
    "    # Return the new lfbfgs class member.\n",
    "    return M_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Macroscopic auto-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macro=False\n",
    "\n",
    "if macro:\n",
    "    \n",
    "    # Annealing and sampling parameters. ======================================================\n",
    "\n",
    "    # Temperature.\n",
    "    T=100.0\n",
    "\n",
    "    # Number of samples.\n",
    "    N_macro=500\n",
    "\n",
    "    # Standard deviation of the randomly generated samples.\n",
    "    sigma_macro=1000.0\n",
    "\n",
    "\n",
    "    # Test functions and LF-BFGS. =============================================================\n",
    "\n",
    "    # Test function class.\n",
    "    fct=testfunctions.f(dim,test_function)\n",
    "\n",
    "    # Initial model (deterministic or randomly chosen).\n",
    "    m=m0.copy()\n",
    "\n",
    "    # Initialise LF-BFGS.\n",
    "    g=fct.J(m)/T\n",
    "    M=lfbfgs(dim,Minv.diagonal(),ell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Actual macroscopic autotuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if macro:\n",
    "\n",
    "    start=time.time()\n",
    "\n",
    "    # Sampling. ===================================================================\n",
    "\n",
    "    for it in range(N_macro-1):\n",
    "\n",
    "        # Check if models and trajectories should be plotted.\n",
    "        if (not it % plot_interval) and it>0: \n",
    "            plot=True\n",
    "            print('iteration: %d' % it)\n",
    "        else:\n",
    "            plot=False\n",
    "\n",
    "        # Make a random new model.\n",
    "        m_new=sigma_macro*np.random.randn(dim)\n",
    "\n",
    "        # Plot proposed models.\n",
    "        if plot:\n",
    "            plt.subplots(1, figsize=(30,10))\n",
    "            plt.plot(M.S0,'k')\n",
    "            plt.xlabel('model parameter index')\n",
    "            plt.grid()\n",
    "            plt.show()\n",
    "\n",
    "        # Evaluate new energies.\n",
    "        U_new=fct.U(m_new)/T\n",
    "\n",
    "        # LF-BFGS update.\n",
    "        s=m_new-m\n",
    "        g_new=fct.J(m_new)/T\n",
    "        y=g_new-g  \n",
    "        if autotune and (not it % update_interval): M.put_sy(s,y)\n",
    "        if autotune and preco and (not it % 5) and it>=ell: M=update_S0(M,S0_min)\n",
    "\n",
    "        # Update model and gradient.\n",
    "        m=m_new\n",
    "        g=g_new\n",
    "\n",
    "    # Output basic statistics.\n",
    "    stop=time.time()\n",
    "    print('elapsed time: %f s' % (stop-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Extract diagonal and rescale to regular mass matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if macro:\n",
    "\n",
    "    Minv=1.0/(M.S0**2)\n",
    "    Minv=T*Minv\n",
    "    \n",
    "else:\n",
    "        \n",
    "    Minv=Minv.diagonal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Leapfrog integrator\n",
    "\n",
    "For clarity, we define the leap-frog integrator as a separate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leapfrog(m,p,Nt,dt,lfbfgs,fct,plot=False):\n",
    "    \"\"\"\n",
    "    Leapfrog time stepping with possible plotting of the trajectory.\n",
    "    \n",
    "    :param m: current model\n",
    "    :param p: current momentum\n",
    "    :param Nt: maximum number of time steps\n",
    "    :param dt: time step lenth\n",
    "    :param lfbfgs: LFBFGS object\n",
    "    :param fct: test function object\n",
    "    :param plot: plot trajectory if True\n",
    "    \"\"\"\n",
    "    \n",
    "    # Plot probability density in the background.\n",
    "    if plot:\n",
    "        fct.plotU(dim,dimension1,dimension2)\n",
    "        plt.plot(m[dimension1],m[dimension2],'bo',MarkerSize=15)\n",
    "    \n",
    "    # Evaluate initial gradient.\n",
    "    J=fct.J(m)\n",
    "    \n",
    "    # Determine randomised integration length.\n",
    "    if Nt>=2: Nti=np.int(Nt*(1.0-0.5*np.random.rand()))\n",
    "    else: Nti=Nt\n",
    "    \n",
    "    # Leapfrog integration.\n",
    "    for k in range(Nti):\n",
    "        \n",
    "        # Save initial model for later plotting and to catch instabilities.\n",
    "        m_old=m.copy()\n",
    "        \n",
    "        # Perform one time step.\n",
    "        p=p-0.5*dt*J\n",
    "        m=m+dt*lfbfgs.Hinv(p)\n",
    "        J=fct.J(m)\n",
    "        p=p-0.5*dt*J\n",
    "        \n",
    "        # Catch potential overflow.\n",
    "        if np.max(np.abs(m))>4000.0:\n",
    "            print('LEAPFROG INSTABILITY')\n",
    "            print(np.max(np.abs(m)))\n",
    "            m=m_old.copy()\n",
    "            break\n",
    "        \n",
    "        # Plot trajectory segment.\n",
    "        if plot: \n",
    "            if k==0: print('number of time steps: %d' % Nti)\n",
    "            plt.plot([m_old[dimension1],m[dimension1]],[m_old[dimension2],m[dimension2]],'r',linewidth=3)\n",
    "            plt.plot(m[dimension1],m[dimension2],'kx',markersize=15)\n",
    "        \n",
    "    return m, p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. HMC initialisations\n",
    "\n",
    "Before running the actual HMC sampler, we perform several initialisations. This includes the test function class, the first random model $\\mathbf{m}$, and the corresponding gradient of the potential energy $\\mathbf{g}=\\nabla U$. With this, we can initialise the BFGS class, which takes $\\mathbf{m}$ and $\\mathbf{g}$ as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation. =============================================================\n",
    "\n",
    "# Test function class.\n",
    "fct=testfunctions.f(dim,test_function)\n",
    "\n",
    "# Initial time step.\n",
    "dt=dt0\n",
    "\n",
    "# Number of accepted models.\n",
    "accept=np.zeros(N)\n",
    "# Current averaged acceptance rate.\n",
    "average_accept=np.ones(N)\n",
    "# Current time step.\n",
    "time_step=dt*np.ones(N)\n",
    "\n",
    "# Initial model (deterministic or randomly chosen).\n",
    "m=m0.copy()\n",
    "\n",
    "# Posterior statistics.\n",
    "stats=samplestatistics.stats(dimension1,dimension2,N)\n",
    "stats.get(m,0.0,0)\n",
    "\n",
    "# Initialise LF-BFGS.\n",
    "g=fct.J(m)\n",
    "M=lfbfgs(dim,Minv,ell)\n",
    "\n",
    "# Unit vectors needed to obtain specific mass matrix components for monitoring.\n",
    "e1=np.zeros(dim)\n",
    "e2=np.zeros(dim)\n",
    "e1[dimension1]=1.0\n",
    "e2[dimension2]=1.0\n",
    "\n",
    "# Last iteration when the adaptive time step was changed.\n",
    "it_change_dt=0\n",
    "\n",
    "# Talk or not.\n",
    "verbose=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run HMC\n",
    "\n",
    "We finally run the HMC sampler. In each iteration, we first produce radom momenta $\\mathbf{p}$ from a normal distribution with covariance chosen to be the BFGS-updated inverse mass matrix $\\mathbf{M}^{-1}$, which is defined to be the inverse Hessian $\\mathbf{H}^{-1}$ of the potential energy $U$. \n",
    "\n",
    "Using the mass matrix, we compute energies and run a leapfrog iteration to solve Hamilton's equations. Following this, we compute the energies of the proposed model and evaluate the modified Metropolis rule (in logarithimic form, to avoid over- or under-flow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dt=0.03\n",
    "\n",
    "start=time.time()\n",
    "\n",
    "# Sampling. ===================================================================\n",
    "\n",
    "for it in range(N-1):\n",
    "\n",
    "    # Randomly choose momentum.\n",
    "    p=np.random.randn(dim)\n",
    "    p=M.S(p)\n",
    "    \n",
    "    # Evaluate energies.\n",
    "    U=fct.U(m)\n",
    "    K=0.5*np.dot(p,M.Hinv(p))\n",
    "    H=U+K\n",
    "    \n",
    "    # Check if models and trajectories should be plotted.\n",
    "    if (not it % plot_interval) and it>0: \n",
    "        plot=True\n",
    "        print('iteration: %d' % it)\n",
    "    else:\n",
    "        plot=False\n",
    "    \n",
    "    # Run leapfrog iteration.\n",
    "    m_new,p_new=leapfrog(m,p,Nit,dt,M,fct,plot)\n",
    "    plt.show()\n",
    "        \n",
    "    # Plot proposed models.\n",
    "    if plot:\n",
    "        plt.subplots(1, figsize=(30,10))\n",
    "        plt.plot(M.S0,'k')\n",
    "        plt.xlabel('model parameter index')\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        \n",
    "    # Evaluate new energies.\n",
    "    U_new=fct.U(m_new)\n",
    "    K_new=0.5*np.dot(p_new,M.Hinv(p_new))\n",
    "    H_new=U_new+K_new\n",
    "    \n",
    "    # Evaluate Metropolis rule in logarithmic form.\n",
    "    alpha=np.minimum(0.0,H-H_new)\n",
    "    if alpha>=np.log(np.random.rand(1)):\n",
    "        # Accepted.\n",
    "        if verbose: print('it=%d accepted' % it)\n",
    "        accept[it]=1\n",
    "        # Update model and gradient.\n",
    "        m=m_new\n",
    "\n",
    "    # Accumulate on-the-fly statistics.\n",
    "    stats.get(m,M.logdet(),it+1)\n",
    "    \n",
    "    # Adaptive time stepping.\n",
    "    Navg=20\n",
    "    if autotune and (it>=Navg) and (it-it_change_dt>Navg/2):\n",
    "        average_accept[it]=np.sum(accept[it-Navg+1:it+1])/np.float(Navg)\n",
    "        time_step[it]=dt\n",
    "        if verbose: print('it=%d average acceptance rate: %f' % (it,average_accept[it]))\n",
    "        if average_accept[it]<0.65:\n",
    "            dt=0.8*dt\n",
    "            time_step[it]=dt\n",
    "            print('--> time step reduced to %f in iteration %d' % (dt,it))\n",
    "            it_change_dt=it\n",
    "        elif average_accept[it]>0.85:\n",
    "            dt=1.25*dt\n",
    "            time_step[it]=dt\n",
    "            print('--> time step increased to %f in iteration %d' % (dt,it))\n",
    "            it_change_dt=it\n",
    "\n",
    "\n",
    "# Output basic statistics.\n",
    "stop=time.time()\n",
    "print('acceptance rate: %f (%d of %d samples)' % (np.sum(accept)/np.float(N),np.sum(accept),N))\n",
    "print('elapsed time: %f s' % (stop-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyse results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. Acceptance and adaptive time stepping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Average acceptance rate.\n",
    "plt.subplots(1, figsize=(20,10))\n",
    "plt.plot(average_accept,'k')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('acceptance rate averaged over %d samples' % Navg)\n",
    "plt.grid()\n",
    "plt.savefig('OUTPUT/acceptance_rate.png', bbox_inches='tight', format='png')\n",
    "plt.show()\n",
    "\n",
    "# Adaptive time step.\n",
    "plt.subplots(1, figsize=(20,10))\n",
    "plt.plot(time_step,'k')\n",
    "plt.xlabel('sample index')\n",
    "plt.ylabel('time step')\n",
    "plt.grid()\n",
    "plt.savefig('OUTPUT/time_step.png', bbox_inches='tight', format='png')\n",
    "plt.show()\n",
    "\n",
    "# Acceptance per sample.\n",
    "plt.subplots(1, figsize=(30,10))\n",
    "plt.plot(accept,'kx')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2. Sample statistics collected on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "stats.display()\n",
    "stats.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3. Analysis of the mass matrix"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# DIAGONAL ELEMENTS OF INVERSE MASS MATRIX\n",
    "Hinv_diag=np.zeros(dim)\n",
    "for i in range(dim):\n",
    "    ei=np.zeros(dim)\n",
    "    ei[i]=1.0\n",
    "    Hinv_diag[i]=np.dot(ei,M.Hinv(ei))\n",
    "    \n",
    "plt.subplots(1, figsize=(20,10))\n",
    "plt.plot(Hinv_diag,'k',linewidth=4)\n",
    "plt.plot(np.diag(Minv),'r',linewidth=4)\n",
    "plt.xlabel('index')\n",
    "plt.title('diagonal of inverse mass matrix (final=black, initial=red)')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# DIAGONAL ELEMENTS OF S\n",
    "Hinv_diag=np.zeros(dim)\n",
    "for i in range(dim):\n",
    "    ei=np.zeros(dim)\n",
    "    ei[i]=1.0\n",
    "    Hinv_diag[i]=np.dot(ei,M.S(ei))\n",
    "    \n",
    "plt.subplots(1, figsize=(20,10))\n",
    "plt.plot(Hinv_diag,'k',linewidth=4)\n",
    "plt.plot(M.S0,'r',linewidth=4)\n",
    "plt.xlabel('index')\n",
    "plt.title('diagonal of S (final=black, initial=red)')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# SPECIFIC ELEMENTS OF THE INVERSE MASS MATRIX\n",
    "plt.subplots(1, figsize=(20,10))\n",
    "plt.plot(m11,'k',linewidth=4)\n",
    "plt.plot(m22,'r',linewidth=4)\n",
    "plt.xlabel('iteration')\n",
    "plt.title('diagonal elements of mass matrix (black=parameter1, red=parameter2)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Initial inverse mass matrix.\n",
    "plt.subplots(1, figsize=(20,20))\n",
    "plt.pcolor(Minv,cmap='Blues')\n",
    "plt.title('initial inverse mass matrix',pad=20)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# Final inverse mass matrix\n",
    "Hinv=np.zeros(np.shape(Minv))\n",
    "for i in range(dim):\n",
    "    ei=np.zeros(dim)\n",
    "    ei[i]=1.0\n",
    "    for j in range(dim):\n",
    "        ej=np.zeros(dim)\n",
    "        ej[j]=1.0\n",
    "        Hinv[i,j]=np.dot(ei,M.Hinv(ej))\n",
    "\n",
    "plt.subplots(1, figsize=(20,20))\n",
    "plt.pcolor(Hinv,cmap='Blues')\n",
    "plt.title('final inverse mass matrix',pad=20)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Actual H\n",
    "H=np.identity(dim)\n",
    "for i in range(dim): H[i,i]=1.0/(0.01+0.99*float(i*dim/(dim*(dim-1))))\n",
    "\n",
    "plt.subplots(1, figsize=(20,20))\n",
    "plt.pcolor(H,cmap='Blues')\n",
    "plt.title('correct Hessian',pad=20)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "# inverse mass matrix times Hessian\n",
    "MinvH=np.dot(Hinv,H)\n",
    "plt.subplots(1, figsize=(20,20))\n",
    "plt.pcolor(MinvH,cmap='Blues')\n",
    "plt.title('MinvH',pad=20)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
