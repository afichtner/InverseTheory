{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "\n",
    "import testfunctions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model space dimension.\n",
    "dim=10\n",
    "# Initial inverse mass matrix.\n",
    "Minv=np.identity(dim)\n",
    "# Number of iterations for the test.\n",
    "it=10\n",
    "# Maximum number of LF-BFGS vectors.\n",
    "k=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Functions for various versions of BFGS updating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Regular BFGS updating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bfgs:\n",
    "    \n",
    "    def __init__(self,dim,Minv,m,g):\n",
    "        \"\"\"\n",
    "        Initialise the BFGS iteration.\n",
    "        \n",
    "        :param dim: number of model-space dimensions\n",
    "        :param Minv: initial inverse mass matrix\n",
    "        :param m: current model vector\n",
    "        :param g: current gradient\n",
    "        \n",
    "        The matrix Minv plays the role of the inverse mass matrix, which ideally is the inverse Hessian, i.e., the covariance matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dim=dim\n",
    "        self.m=m\n",
    "        self.g=g\n",
    "        \n",
    "        # Initial mass matrix.\n",
    "        self.Minv=Minv\n",
    "        \n",
    "        # Initial factorisation.\n",
    "        LT=np.linalg.cholesky(self.Minv).transpose()\n",
    "        self.LTinv=np.linalg.inv(LT)\n",
    "        \n",
    "        \n",
    "    def update(self,m,g):\n",
    "        \"\"\"\n",
    "        Update BFGS matrix and perform Cholesky decomposition.\n",
    "        \n",
    "        :param m: current model vector\n",
    "        :param g: current gradient\n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute differences and update vectors.\n",
    "        s=m-self.m\n",
    "        y=g-self.g\n",
    "\n",
    "        if np.dot(s,y)>0:\n",
    "        \n",
    "            self.m=m\n",
    "            self.g=g\n",
    "        \n",
    "            # Compute update of BFGS matrix.\n",
    "            rho=1.0/np.dot(s,y)\n",
    "            I=np.identity(self.dim)\n",
    "            sy=rho*np.tensordot(s,y,axes=0)\n",
    "            ss=rho*np.tensordot(s,s,axes=0)\n",
    "            self.Minv=np.matmul(np.matmul((I-sy),self.Minv),(I-sy.transpose()))+ss\n",
    "        \n",
    "            # Compute Cholesky decomposition.\n",
    "            LT=np.linalg.cholesky(self.Minv).transpose()\n",
    "            self.LTinv=np.linalg.inv(LT)\n",
    "            \n",
    "        else: \n",
    "            rhoinv=np.dot(s,y)\n",
    "            print('BFGS check failed (1/rho=%f)' % rhoinv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Factorised BFGS updating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fbfgs:\n",
    "    \n",
    "    def __init__(self,dim,Minv,m,g,d_max):\n",
    "        \"\"\"\n",
    "        Initialise the BFGS iteration.\n",
    "        \n",
    "        :param dim: number of model-space dimensions\n",
    "        :param Minv: initial inverse mass matrix (must be diagonal)\n",
    "        :param m: current model vector\n",
    "        :param g: current gradient\n",
    "        :param d_max: maximum change of the determinant of the Hessian factor S in of F-BFGS update\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dim=dim   # Model space dimension.\n",
    "        self.Hinv=Minv   # Initial (current) estimate of the inverse Hessian H^{-1}.\n",
    "        \n",
    "        # Initial (current) matrix factor S and estimate of the Hessian\n",
    "        \n",
    "        self.S=np.identity(dim)   \n",
    "        self.H=np.identity(dim)   \n",
    "        \n",
    "        for i in range(dim):\n",
    "            self.H[i,i]=1.0/Minv[i,i]\n",
    "            self.S[i,i]=1.0/np.sqrt(Minv[i,i])\n",
    "        \n",
    "        self.logdet=0.0   # Initial (current) logarithm of the determinant of the Hessian H.\n",
    "        self.m=m   # Initial (current) model.\n",
    "        self.g=g   # Initial (current) gradient.\n",
    "        self.I=np.identity(dim)   # Identity matrix. \n",
    "        \n",
    "        self.sigma=d_max   # Minimum factorial change of determinant per update.\n",
    "        \n",
    "    def update(self,m,g):\n",
    "        \"\"\"\n",
    "        Update BFGS matrix and its factorised form.\n",
    "        \n",
    "        :param m: current model vector\n",
    "        :param g: current gradient\n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute differences and update vectors.\n",
    "        s=m-self.m\n",
    "        y=g-self.g\n",
    "        \n",
    "        # Do nothing unless rho is positive.\n",
    "        if np.dot(s,y)>0:\n",
    "            \n",
    "            # Set new to current vectors.\n",
    "            self.m=m\n",
    "            self.g=g\n",
    "            \n",
    "            # Precompute inverse Hessian-vector product.\n",
    "            Hinv_y=np.dot(self.Hinv,y)\n",
    "            \n",
    "            # Auxiliary scalars.\n",
    "            rho=1.0/np.dot(s,y)\n",
    "            gamma2=rho**2 * np.dot(y,Hinv_y) + rho\n",
    "            beta=gamma2 * np.dot(s,np.dot(self.H,s))\n",
    "            theta=-np.sqrt(rho/(beta*gamma2))\n",
    "        \n",
    "            # Auxiliary vectors a, b, and u, v.\n",
    "            a=np.sqrt(gamma2)*s\n",
    "            b=(rho/np.sqrt(gamma2))*Hinv_y\n",
    "            u=a\n",
    "            v=-np.dot(self.H,b+theta*a)\n",
    "            \n",
    "            # Auxiliary scalar for regularisation.\n",
    "            #sigma_threshold=(1.0/(1.0+np.dot(u,v)))**2\n",
    "            \n",
    "            # Scaling of v for regularisation.\n",
    "            #if sigma_threshold<self.sigma:\n",
    "            #    r=(1.0-self.sigma)/(self.sigma*np.dot(u,v))\n",
    "            #    v=r*v\n",
    "        \n",
    "            # Update inverse Hessian estimate Hinv. \n",
    "            alpha=1.0/(1.0+np.dot(u,v))\n",
    "            Hinv_v=np.dot(self.Hinv,v)\n",
    "            self.Hinv=self.Hinv+np.tensordot(Hinv_v,u,axes=0)+np.tensordot(u,Hinv_v,axes=0)+np.tensordot(u,u,axes=0)*np.dot(v,Hinv_v)\n",
    "        \n",
    "            # Update Hessian estimate H.\n",
    "            H_u=np.dot(self.H,u)\n",
    "            self.H=self.H-alpha*np.tensordot(H_u,v,axes=0)-alpha*np.tensordot(v,H_u,axes=0)+(alpha**2)*np.tensordot(v,v,axes=0)*np.dot(u,H_u)\n",
    "        \n",
    "            # Update factor S.\n",
    "            ST_u=np.dot(self.S.transpose(),u)\n",
    "            self.S=self.S-alpha*np.tensordot(v,ST_u,axes=0)\n",
    "            \n",
    "            # Update determinant of S.\n",
    "            self.logdet=self.logdet + np.log(alpha**2)\n",
    "            \n",
    "        else: \n",
    "            rhoinv=np.dot(s,y)\n",
    "            print('F-BFGS check failed (1/rho=%f)' % rhoinv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Limited-memory factorised BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lfbfgs:\n",
    "    \n",
    "    # Initialisation. ==================================================================\n",
    "    \n",
    "    def __init__(self,dim,Minv,k,m,g,d_max):\n",
    "        \"\"\"\n",
    "        Initialise the LF-BFGS iteration.\n",
    "        \n",
    "        :param dim: number of model-space dimensions\n",
    "        :param Minv: diagonal elements of the initial inverse mass matrix\n",
    "        :param k: maximum number of vectors to be stored\n",
    "        :param m: current model vector\n",
    "        :param g: current gradient\n",
    "        :param d_max: maximum change of the determinant of the Hessian factor S in of F-BFGS update\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dim=dim # Model space dimension.\n",
    "        self.k=k # Maximum number of vectors.\n",
    "        self.i=0 # Currently stored vectors.\n",
    "        \n",
    "        self.m=m # Initial (current) model.\n",
    "        self.g=g # Initial (current) gradient.\n",
    "        \n",
    "        self.s=np.zeros((dim,k)) # s vectors.\n",
    "        self.y=np.zeros((dim,k)) # y vectors.\n",
    "        self.u=np.zeros((dim,k)) # u vectors.\n",
    "        self.v=np.zeros((dim,k)) # v vectors.\n",
    "        self.vTu=np.zeros(k) # Precomputed 1+vT*u.\n",
    "        \n",
    "        self.S0=np.ones(dim) # Diagonal components of the initial S factor.\n",
    "        for i in range(dim):\n",
    "            self.S0[i]=1.0/np.sqrt(Minv[i])\n",
    "        \n",
    "        self.sigma=d_max   # Minimum factorial change of determinant per update.\n",
    "        \n",
    "    # Compute and store new vectors u and v. ===========================================\n",
    "        \n",
    "    def put_uv(self,m,g):\n",
    "        \"\"\"\n",
    "        Put in a new pair of vectors u and v.\n",
    "        \n",
    "        :param m: current model vector\n",
    "        :param g: current gradient\n",
    "        \"\"\"\n",
    "        \n",
    "        #===============================================================================\n",
    "        # Compute new s, y, u, v vectors.\n",
    "        #===============================================================================\n",
    "        \n",
    "        # Compute differences to get the current s and y.\n",
    "        s=m-self.m\n",
    "        y=g-self.g\n",
    "        \n",
    "        sy=np.dot(s,y)\n",
    "        \n",
    "        # Do nothing unless rho is positive.\n",
    "        if sy>0:\n",
    "            \n",
    "            # Set new to current vectors.\n",
    "            self.m=m\n",
    "            self.g=g\n",
    "            \n",
    "            # Precompute inverse Hessian-vector product.\n",
    "            Hinv_y=self.Hinv(y)\n",
    "        \n",
    "            # Auxiliary scalars.\n",
    "            rho=1.0/sy\n",
    "            gamma2=rho**2 * np.dot(y,Hinv_y) + rho\n",
    "            beta=gamma2 * np.dot(s,self.H(s))\n",
    "            theta=-np.sqrt(rho/(beta*gamma2)) # Here, one may also choose the positive square root. Empirically, the negative ones works much better because it leads to a diagonally dominant S, which is better for preconditioning.\n",
    "        \n",
    "            # Auxiliary vectors a and b.\n",
    "            a=np.sqrt(gamma2)*s\n",
    "            b=(rho/np.sqrt(gamma2))*Hinv_y\n",
    "        \n",
    "            # Compute the current u and v.\n",
    "            u=a\n",
    "            v=-self.H(b+theta*a)\n",
    "            \n",
    "            # Auxiliary scalar for regularisation.\n",
    "            #sigma_threshold=(1.0/(1.0+np.dot(u,v)))**2\n",
    "            \n",
    "            # Scaling of v for regularisation.\n",
    "            #if sigma_threshold<self.sigma:\n",
    "            #    r=(1.0-self.sigma)/(self.sigma*np.dot(u,v))\n",
    "            #    v=r*v\n",
    "            \n",
    "            #===============================================================================\n",
    "            # Update s, y, u, v in the memory.\n",
    "            #===============================================================================\n",
    "        \n",
    "            # When less then k vector pairs are stored, we increase the index of stored pairs and add the new pair.\n",
    "            if self.i<self.k:\n",
    "                self.i+=1\n",
    "            # Otherwise we kick out the pair with the lowest index by rolling to the left and over-writing the last pair.\n",
    "            else:\n",
    "                self.s=np.roll(self.s, -1, axis=1)\n",
    "                self.y=np.roll(self.y, -1, axis=1)\n",
    "                self.u=np.roll(self.u, -1, axis=1)\n",
    "                self.v=np.roll(self.v, -1, axis=1)\n",
    "                self.vTu=np.roll(self.vTu,-1)\n",
    "        \n",
    "            self.s[:,self.i-1]=s\n",
    "            self.y[:,self.i-1]=y\n",
    "            self.u[:,self.i-1]=u\n",
    "            self.v[:,self.i-1]=v\n",
    "            self.vTu[self.i-1]=1.0+np.dot(v,u)\n",
    "            \n",
    "        else: \n",
    "            print('LF-BFGS check failed (1/rho=%f)' % sy)\n",
    "            \n",
    "    # Update diagonal elements of the initial factor S0. ===============================\n",
    "\n",
    "    def update_S0(self):\n",
    "        \"\"\"\n",
    "        Update the diagonal elements of the initial factor S0 using the diagonal elements of the current approximation.\n",
    "        \"\"\"\n",
    "    \n",
    "        #p=np.ones(self.dim)\n",
    "        #self.S0=np.abs(self.S(p))\n",
    "        #print(np.min(self.S0))\n",
    "    \n",
    "        S0=np.zeros(self.dim)\n",
    "        for i in range(self.dim):\n",
    "            p=np.zeros(self.dim)\n",
    "            p[i]=1.0\n",
    "            S0[i]=self.S(p)[i]\n",
    "        \n",
    "        #if np.min(S0) < 0.0: S0=np.ones(self.dim)\n",
    "        print(\"minS0=%f\" % np.min(S0))\n",
    "        self.S0=S0\n",
    "        \n",
    "    # Reset diagonal elements of the initial factor S0. ===============================\n",
    "    \n",
    "    def reset_S0(self):\n",
    "        \"\"\"\n",
    "        Reset the diagonal elements of S0 to 1.\n",
    "        \"\"\"\n",
    "        self.S0=np.ones(dim)\n",
    "            \n",
    "\n",
    "    # Implement matrix-vector products. ===============================================\n",
    "    \n",
    "    def S(self,h):\n",
    "        \"\"\"\n",
    "        S*h, product of factor S with some vector h\n",
    "        :param h: vector of dimension self.dim\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(self.i):\n",
    "            h=h-self.v[:,i]*np.dot(self.u[:,i],h)/self.vTu[i]\n",
    "        return h*self.S0\n",
    "    \n",
    "    def Sn(self,h,n):\n",
    "        \"\"\"\n",
    "        S*h, product of factor S with some vector h using a smaller number n of vectors\n",
    "        :param h: vector of dimension self.dim\n",
    "        :param n: number of vectors to take into account\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(self.i):\n",
    "            h=h-self.v[:,i]*np.dot(self.u[:,i],h)/self.vTu[i]\n",
    "        return h*self.S0\n",
    "    \n",
    "    def ST(self,h):\n",
    "        \"\"\"\n",
    "        S^T*h, product of transposed factor S with some vector h\n",
    "        :param h: vector of dimension self.dim\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(self.i-1,-1,-1):\n",
    "            h=h-self.u[:,i]*np.dot(self.v[:,i],h)/self.vTu[i]\n",
    "        return h*self.S0\n",
    "        \n",
    "    def Sinv(self,h):\n",
    "        \"\"\"\n",
    "        S^-1*h, product of inverse factor S with some vector h\n",
    "        :param h: vector of dimension self.dim\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(self.i-1,-1,-1):\n",
    "            h=h+self.v[:,i]*np.dot(self.u[:,i],h)\n",
    "        return h/self.S0\n",
    "    \n",
    "    def SinvT(self,h):\n",
    "        \"\"\"\n",
    "        S^-T*h, product of inverse transposed factor S with some vector h\n",
    "        :param h: vector of dimension self.dim\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in range(self.i):\n",
    "            h=h+self.u[:,i]*np.dot(self.v[:,i],h)\n",
    "        return h/self.S0\n",
    "    \n",
    "    def H(self,h):\n",
    "        \"\"\"\n",
    "        H*h, product of Hessian approximation H with some vector h\n",
    "        :param h: vector of dimension self.dim\n",
    "        \"\"\"\n",
    "        \n",
    "        h=self.ST(h)\n",
    "        return self.S(h)\n",
    "    \n",
    "    def Hinv(self,h):\n",
    "        \"\"\"\n",
    "        Hinv*h, product of inverse Hessian approximation H with some vector h\n",
    "        :param h: vector of dimension self.dim\n",
    "        \"\"\"\n",
    "        \n",
    "        h=self.Sinv(h)\n",
    "        return self.SinvT(h)\n",
    "    \n",
    "    # Implement log determinant of S. ==================================================\n",
    "    def logdet(self):\n",
    "        \"\"\"\n",
    "        Current log of the determinant of S\n",
    "        \"\"\"\n",
    "        \n",
    "        logdet=0.0\n",
    "        for i in range(self.i):\n",
    "            alpha=1.0/(1.0+np.dot(self.u[:,i],self.v[:,i]))\n",
    "            logdet+=np.log(alpha**2)\n",
    "            \n",
    "        return logdet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Run test by generating sequence of $m$ and $g$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial m and g.\n",
    "m=np.random.randn(dim)\n",
    "g=np.random.randn(dim)\n",
    "\n",
    "# Initialise BFGS updating schemes.\n",
    "M_bfgs=bfgs(dim,Minv,m,g)\n",
    "M_fbfgs=fbfgs(dim,Minv,m,g,1.0)\n",
    "M_lfbfgs=lfbfgs(dim,Minv.diagonal(),k,m,g,1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(it):\n",
    "    # New m and g.\n",
    "    m_new=np.random.randn(dim)\n",
    "    g_new=np.random.randn(dim)\n",
    "    # Check updating condition.\n",
    "    rhoinv=np.dot(m-m_new,g-g_new)\n",
    "    print('1/rho=%f' % rhoinv)\n",
    "    # Update\n",
    "    M_bfgs.update(m_new,g_new)\n",
    "    M_fbfgs.update(m_new,g_new)\n",
    "    M_lfbfgs.put_uv(m_new,g_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Analyse output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random test vector.\n",
    "p=np.random.randn(dim)\n",
    "\n",
    "# Compute products with current mass matrix M.\n",
    "prod_bfgs=np.dot(M_bfgs.LTinv,np.dot(M_bfgs.LTinv.transpose(),p))\n",
    "prod_fbfgs=np.dot(M_fbfgs.H,p)\n",
    "prod_lfbfgs=M_lfbfgs.H(p)\n",
    "\n",
    "offset=np.max(np.abs(prod_bfgs))/50.0\n",
    "\n",
    "plt.plot(prod_bfgs,'k')\n",
    "plt.plot(prod_fbfgs+1.0*offset,'b')\n",
    "plt.plot(prod_lfbfgs+2.0*offset,'r')\n",
    "plt.show()\n",
    "\n",
    "print(prod_bfgs)\n",
    "print(prod_fbfgs)\n",
    "print(prod_lfbfgs)\n",
    "\n",
    "# Compute products with inverse of current mass matrix M^-1.\n",
    "prod_bfgs=np.dot(M_bfgs.Minv,p)\n",
    "prod_fbfgs=np.dot(M_fbfgs.Hinv,p)\n",
    "prod_lfbfgs=M_lfbfgs.Hinv(p)\n",
    "\n",
    "offset=np.max(np.abs(prod_bfgs))/50.0\n",
    "\n",
    "plt.plot(prod_bfgs,'k')\n",
    "plt.plot(prod_fbfgs+1.0*offset,'b')\n",
    "plt.plot(prod_lfbfgs+1.0*offset,'r')\n",
    "plt.show()\n",
    "\n",
    "print(prod_bfgs)\n",
    "print(prod_fbfgs)\n",
    "print(prod_lfbfgs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
