{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMC in ND with BFGS or SR1 autotuning of the mass matrix\n",
    "\n",
    "This notebook implements an autotuning HMC for an N-dimensional distribution based on BFGS or SR1 updating of the inverse Hessian. The SR1 updating is not possible in the factorised versions because it is not positive definite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "\n",
    "import testfunctions\n",
    "import samplestatistics\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Times\"\n",
    "plt.rcParams.update({'font.size': 50})\n",
    "plt.rcParams['xtick.major.pad']='12'\n",
    "plt.rcParams['ytick.major.pad']='12'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Input\n",
    "\n",
    "We first define several input parameters, including the model space dimension, the initial inverse mass matrix $\\mathbf{M}^{-1}$, the total number of samples, the number of leapfrog timesteps, and the length of the timestep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import input_parameters\n",
    "reload(input_parameters)\n",
    "test_function,dim,N,Nit,dt,m0,Minv,autotune,ell,update_interval,preco,S0_min,plot_interval,dimension1,dimension2,m1_min,m1_max,m2_min,m2_max=input_parameters.input_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classes for quasi-Newton autotuning\n",
    "\n",
    "For later convenience, we introduce classes that perform quasi-Newton updating of the inverse Hessian $\\mathbf{H}^{-1}$, which serves as inverse mass matrix, $\\mathbf{M}^{-1}$. The classes performs an update of $\\mathbf{M}^{-1}$ at each new sample and then computes the Cholesky decomposition of the update. The class is written for arbitrary dimension, but it is clear that the brute-force Cholesky decomposition will only be feasible for low dimensions. (For higher dimensions, the Cholesky decomposition will still yield some output, but it will not be very accurate and useful.) \n",
    "\n",
    "**Call to caution**: There is an experimental component in this class. In principle, BFGS updates can only be made when $\\mathbf{s}_k^T\\mathbf{y}>0$. However, when this quantity is very small, the resulting Hessian approximation may still be close to singular. Empirically, it is better for stability to choose $\\mathbf{s}_k^T\\mathbf{y}>\\gamma$, with some tuning parameter $\\gamma>0$. For many examples, $\\gamma=2$ works very well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. BFGS updating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bfgs:\n",
    "    \n",
    "    def __init__(self,dim,Minv,m,g):\n",
    "        \"\"\"\n",
    "        Initialise the BFGS iteration.\n",
    "        \n",
    "        :param dim: number of model-space dimensions\n",
    "        :param Minv: initial inverse mass matrix\n",
    "        :param m: current model vector\n",
    "        :param g: current gradient\n",
    "        \n",
    "        The matrix Minv plays the role of the inverse mass matrix, which ideally is the inverse Hessian, i.e., the covariance matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dim=dim\n",
    "        self.m=m\n",
    "        self.g=g\n",
    "        \n",
    "        # Initial mass matrix.\n",
    "        self.Minv=Minv\n",
    "        \n",
    "        # Initial factorisation.\n",
    "        LT=np.linalg.cholesky(self.Minv).transpose()\n",
    "        self.LTinv=np.linalg.inv(LT)\n",
    "        \n",
    "        \n",
    "    def update(self,m,g):\n",
    "        \"\"\"\n",
    "        Update BFGS matrix and perform Cholesky decomposition.\n",
    "        \n",
    "        :param m: current model vector\n",
    "        :param g: current gradient\n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute differences and update vectors.\n",
    "        s=m-self.m\n",
    "        y=g-self.g\n",
    "        \n",
    "        # BFGS check.\n",
    "        check=np.dot(s,y)\n",
    "        print(check)\n",
    "\n",
    "        if check>2.0:\n",
    "        \n",
    "            self.m=m\n",
    "            self.g=g\n",
    "        \n",
    "            # Compute update of BFGS matrix.\n",
    "            rho=1.0/np.dot(s,y)\n",
    "            I=np.identity(self.dim)\n",
    "            sy=rho*np.tensordot(s,y,axes=0)\n",
    "            ss=rho*np.tensordot(s,s,axes=0)\n",
    "            self.Minv=np.matmul(np.matmul((I-sy),self.Minv),(I-sy.transpose()))+ss\n",
    "        \n",
    "            # Compute Cholesky decomposition.\n",
    "            LT=np.linalg.cholesky(self.Minv).transpose()\n",
    "            self.LTinv=np.linalg.inv(LT)\n",
    "            \n",
    "        else: \n",
    "            rhoinv=np.dot(s,y)\n",
    "            print('BFGS check failed (1/rho=%f)' % rhoinv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. SR1 updating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sr1:\n",
    "    \n",
    "    def __init__(self,dim,Minv,m,g):\n",
    "        \"\"\"\n",
    "        Initialise the SR1 iteration.\n",
    "        \n",
    "        :param dim: number of model-space dimensions\n",
    "        :param Minv: initial mass matrix inverse \n",
    "        :param m: current model vector\n",
    "        :param g: current gradient\n",
    "        \n",
    "        The matrix Minv plays the role of the inverse mass matrix, which ideally is the inverse Hessian, i.e., the covariance matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.dim=dim\n",
    "        self.m=m\n",
    "        self.g=g\n",
    "        \n",
    "        # Initial mass matrix.\n",
    "        self.Minv=Minv\n",
    "        \n",
    "        # Initial factorisation.\n",
    "        LT=np.linalg.cholesky(self.Minv).transpose()\n",
    "        self.LTinv=np.linalg.inv(LT)\n",
    "        \n",
    "    def update(self,m,g):\n",
    "        \"\"\"\n",
    "        Update SR1 matrix and perform Cholesky decomposition.\n",
    "        \n",
    "        :param m: current model vector\n",
    "        :param g: current gradient\n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute differences and update vectors.\n",
    "        s=m-self.m\n",
    "        y=g-self.g\n",
    "        \n",
    "        self.m=m\n",
    "        self.g=g\n",
    "        \n",
    "        # Compute update of SR1 matrix.\n",
    "        Ay=np.dot(self.Minv,y)\n",
    "        x=s-Ay\n",
    "        n=np.dot(x,y)\n",
    "        if np.abs(n)>0.01*np.linalg.norm(x)*np.linalg.norm(y):\n",
    "            self.Minv=self.Minv+np.tensordot(x,x,axes=0)/n\n",
    "                    \n",
    "            # Compute Cholesky decomposition.\n",
    "            LT=np.linalg.cholesky(self.Minv).transpose()\n",
    "            self.LTinv=np.linalg.inv(LT)\n",
    "            \n",
    "        else: print('check failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Leapfrog integrator\n",
    "\n",
    "For clarity, we define the leap-frog integrator as a separate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leapfrog(m,p,Nt,dt,Minv,fct,plot=False):\n",
    "    \n",
    "    # Plot probability density in the background.\n",
    "    if plot:\n",
    "        fct.plotU(dim,dimension1,dimension2,m1_min,m1_max,m2_min,m2_max)\n",
    "        plt.plot(m[dimension1],m[dimension2],'bo',MarkerSize=15)\n",
    "    \n",
    "    # Evaluate initial gradient.\n",
    "    J=fct.J(m)\n",
    "    \n",
    "    # Determine randomised integration length.\n",
    "    Nti=np.int(Nt*(1.0-0.5*np.random.rand()))\n",
    "    \n",
    "    # Leapfrog integration.\n",
    "    for k in range(Nti):\n",
    "        \n",
    "        if plot: m_old=m.copy()\n",
    "        \n",
    "        p=p-0.5*dt*J\n",
    "        m=m+dt*Minv.dot(p)\n",
    "        J=fct.J(m)\n",
    "        p=p-0.5*dt*J\n",
    "        \n",
    "        # Plot trajectory segment.\n",
    "        if plot: \n",
    "            if k==0: print('number of time steps: %d' % Nti)\n",
    "            plt.plot([m_old[dimension1],m[dimension1]],[m_old[dimension2],m[dimension2]],'r',Linewidth=3)\n",
    "            plt.plot(m[dimension1],m[dimension2],'kx')\n",
    "        \n",
    "    return m, p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. HMC initialisations\n",
    "\n",
    "Before running the actual HMC sampler, we perform several initialisations. This includes the test function class, the first random model $\\mathbf{m}$, and the corresponding gradient of the potential energy $\\mathbf{g}=\\nabla U$. With this, we can initialise the BFGS class, which takes $\\mathbf{m}$ and $\\mathbf{g}$ as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation. =============================================================\n",
    "\n",
    "# Test function class.\n",
    "fct=testfunctions.f(dim,test_function)\n",
    "\n",
    "# Number of accepted models.\n",
    "accept=0\n",
    "\n",
    "# Initial model.\n",
    "m=m0\n",
    "\n",
    "# Posterior statistics.\n",
    "s=samplestatistics.stats(dimension1,dimension2,N)\n",
    "s.get(m,0.0,0)\n",
    "\n",
    "# Initialise BFGS matrix.\n",
    "g=fct.J(m)\n",
    "if autotune=='BFGS':\n",
    "    M=bfgs(dim,Minv,m,g)\n",
    "else:\n",
    "    M=sr1(dim,Minv,m,g)\n",
    "    \n",
    "m11=Minv[dimension1,dimension1]*np.ones(N)\n",
    "m22=Minv[dimension2,dimension2]*np.ones(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fct.plotU(dim,dimension1,dimension2,m1_min,m1_max,m2_min,m2_max)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "w,v=np.linalg.eig(fct.Cinv)\n",
    "print(w)\n",
    "print(2.0/np.sqrt(10.0))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(fct.Cinv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run HMC\n",
    "\n",
    "We finally run the HMC sampler. In each iteration, we first produce radom momenta $\\mathbf{p}$ from a normal distribution with covariance chosen to be the BFGS-updated inverse mass matrix $\\mathbf{M}^{-1}$, which is defined to be the inverse Hessian $\\mathbf{H}^{-1}$ of the potential energy $U$. \n",
    "\n",
    "Using the mass matrix, we compute energies and run a leapfrog iteration to solve Hamilton's equations. Following this, we compute the energies of the proposed model and evaluate the modified Metropolis rule (in logarithimic form, to avoid over- or under-flow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "accept=0\n",
    "start=time.time()\n",
    "\n",
    "# Sampling. ===================================================================\n",
    "for it in range(N-1):\n",
    "    \n",
    "    # Randomly choose momentum.\n",
    "    p=np.random.randn(dim)\n",
    "    p=M.LTinv.dot(p)\n",
    "    \n",
    "    # Evaluate energies.\n",
    "    U=fct.U(m)\n",
    "    K=0.5*np.dot(p,np.dot(M.Minv,p))\n",
    "    H=U+K\n",
    "    \n",
    "    # Check if models and trajectories should be plotted.\n",
    "    if (not it % plot_interval) and it>0: \n",
    "        plot=True\n",
    "        print('iteration: %d' % it)\n",
    "    else:\n",
    "        plot=False\n",
    "    \n",
    "    # Run leapfrog iteration.\n",
    "    m_new,p_new=leapfrog(m,p,Nit,dt,M.Minv,fct,plot)\n",
    "    if plot:\n",
    "        filename='OUTPUT/trajectory_'+str(it)+'.png'\n",
    "        plt.savefig(filename, bbox_inches='tight', format='png')\n",
    "        plt.show()\n",
    "    \n",
    "    # Plot proposed models.\n",
    "    if plot:\n",
    "        plt.subplots(1, figsize=(30,10))\n",
    "        plt.plot(m_new)\n",
    "        plt.xlabel('model parameter index')\n",
    "        plt.show()\n",
    "    \n",
    "    # Evaluate new energies.\n",
    "    U_new=fct.U(m_new)\n",
    "    K_new=0.5*np.dot(p_new,M.Minv.dot(p_new))\n",
    "    H_new=U_new+K_new\n",
    "    \n",
    "    # Evaluate Metropolis rule in logarithmic form.\n",
    "    alpha=np.minimum(0.0,H-H_new)\n",
    "    if alpha>=np.log(np.random.rand(1)):\n",
    "        # Update model.\n",
    "        m=m_new\n",
    "        accept+=1\n",
    "        # Update BFGS matrix.\n",
    "        if (autotune=='BFGS' or autotune=='SR1'):\n",
    "            g=fct.J(m)\n",
    "            M.update(m,g)\n",
    "    \n",
    "    # Accumulate on-the-fly statistics\n",
    "    s.get(m,0.0,it+1)\n",
    "    m11[it+1]=M.Minv[dimension1,dimension1]\n",
    "    m22[it+1]=M.Minv[dimension2,dimension2]\n",
    "\n",
    "stop=time.time()\n",
    "print('acceptance rate: %f (%d of %d samples)' % (np.float(accept)/np.float(N),accept,N))\n",
    "print('elapsed time: %f s' % (stop-start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyse results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1. Sample statistics collected on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u,v=np.linalg.eig(M.Minv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(1, figsize=(20,20))\n",
    "plt.semilogy(np.abs(u))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2. Analysis of the mass matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.subplots(1, figsize=(20,20))\n",
    "plt.pcolor(Minv,cmap='Blues')\n",
    "plt.title('initial inverse mass matrix',pad=20)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(1, figsize=(20,20))\n",
    "plt.pcolor(M.Minv,cmap='Blues')\n",
    "plt.title('final inverse mass matrix',pad=20)\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(1, figsize=(20,10))\n",
    "plt.plot(np.diag(M.Minv),'k',linewidth=4)\n",
    "plt.plot(np.diag(Minv),'r',linewidth=4)\n",
    "plt.xlabel('index')\n",
    "plt.title('diagonal of inverse mass matrix (final=black, initial=red)')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(1, figsize=(20,10))\n",
    "plt.plot(m11,'k',linewidth=4)\n",
    "plt.plot(m22,'r',linewidth=4)\n",
    "plt.xlabel('iteration')\n",
    "plt.title('diagonal elements (black=parameter1, red=parameter2)')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
