{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nonlinear optimisation in 2D\n",
    "\n",
    "The following notebook provides examples of several nonlinear optimisation methods (steepest descent, Newton, different conjugate-gradient variants, BFGS, and L-BFGS) applied to various 2D test functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import some Python packages\n",
    "\n",
    "We begin by importing some Python packages for matrix-vector operations, for plotting, and for a collection of 2D test functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Python packages.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import test_functions as f\n",
    "\n",
    "# Set some parameters to make plots nicer.\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams.update({'font.size': 70})\n",
    "plt.rcParams['xtick.major.pad']='20'\n",
    "plt.rcParams['ytick.major.pad']='20'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Input parameters and initialisation\n",
    "\n",
    "In the following, we set a few input parameters, including the initial coordinates, the initial step length for the line search, the line search method, the optimisation method, and the test function we would like to use. Currently implemented test functions are:\n",
    "\n",
    "\\begin{equation}\n",
    "f(x,y) = (1.0-x)^2 + 100.0 *(y-x^2)^2\\,,\\qquad \\text{(Rosenbrock function)}\\,,\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "f(x,y) = x^2 + y^2 + xy\\,,\\qquad \\text{(quadratic function)}\\,,\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "f(x,y) = (x^2 + y - 11.0)^2 + (x + y^2 - 7.0)^2\\,,\\qquad \\text{(Himmelblau function)}\\,,\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "f(x,y) = (x-2.0)^4 + (x - 2.0y)^2\\,,\\qquad \\text{(Bazaraa-Shetty function)}\\,.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input parameters ------------------------------------------------------------------------\n",
    "\n",
    "# Initial coordinates.\n",
    "x0=-0.55\n",
    "y0=0.55\n",
    "\n",
    "# Initial and minimum step length. Serves as step length increment in exhaustive, full line search.\n",
    "s_min=0.001\n",
    "\n",
    "# Test function ('rosenbrock', 'quadratic', 'himmelblau', 'bazaraa-shetty').\n",
    "function='bazaraa-shetty'\n",
    "\n",
    "# Line search method ('quadratic', 'full')\n",
    "ls_method='full'\n",
    "\n",
    "# Number of iterations.\n",
    "nit=30\n",
    "\n",
    "# Print misfit.\n",
    "print_misfit=False\n",
    "\n",
    "# Save figures.\n",
    "save_figure=True\n",
    "\n",
    "# Initialisations -------------------------------------------------------------------------\n",
    "\n",
    "# Useful plotting domains and optima for the test functions.\n",
    "if function=='quadratic':\n",
    "    xmin=-2.1\n",
    "    xmax=2.1\n",
    "    ymin=-2.1\n",
    "    ymax=2.1\n",
    "    xopt=0.0\n",
    "    yopt=0.0\n",
    "elif function=='rosenbrock':\n",
    "    xmin=-1.1\n",
    "    xmax=1.1\n",
    "    ymin=-1.5\n",
    "    ymax=1.5\n",
    "    xopt=1.0\n",
    "    yopt=1.0\n",
    "elif function=='himmelblau':\n",
    "    xmin=-5.5\n",
    "    xmax=5.5\n",
    "    ymin=-5.5\n",
    "    ymax=7.5\n",
    "    xopt=-2.80511809\n",
    "    yopt=3.13131252\n",
    "    # There are three other minima!\n",
    "elif function=='bazaraa-shetty':\n",
    "    xmin=1.5\n",
    "    xmax=2.6\n",
    "    ymin=0.8\n",
    "    ymax=1.3\n",
    "    xopt=2.0\n",
    "    yopt=1.0\n",
    "\n",
    "# Make domain for plotting.\n",
    "xp,yp=np.meshgrid(np.linspace(xmin,xmax,100),np.linspace(ymin,ymax,100))\n",
    "\n",
    "# Initialise coordinates. ---------------------------------------------------------------\n",
    "opt=np.matrix([[xopt],[yopt]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Line search and updating\n",
    "\n",
    "Before implementing any specific optimisation method, we write a little helper function that performs a line search and updates the current position. The function takes the current position $\\mathbf{x}$, the descent direction $\\mathbf{h}$, and the initial step length $s$.\n",
    "\n",
    "Two line search options are available: \n",
    "\n",
    "(1) A *full* line search where a large number of step lengths are tested. In this case, the initial step length $s$ also serves as the step length increment.\n",
    "\n",
    "(2) A *quadratic* line search where the misfit along the search direction is approximated by a quadratic curve fitted to three trial step lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_search(x,h,s,function=\"rosenbrock\",ls_method=\"quadratic\"):\n",
    "    \n",
    "    # Quadratic interpolation. ---------------------------\n",
    "    if ls_method==\"quadratic\":\n",
    "        \n",
    "        # Trial models. \n",
    "        x0=x\n",
    "        chi0=f.f(x0[0,0],x0[1,0],function)\n",
    "        \n",
    "        s1=s\n",
    "        x1=x+s1*h\n",
    "        chi1=f.f(x1[0,0],x1[1,0],function)\n",
    "        \n",
    "        s2=2.0*s\n",
    "        x2=x+s2*h\n",
    "        chi2=f.f(x2[0,0],x2[1,0],function)\n",
    "        \n",
    "        #- Polynomial coefficients and optimum. \n",
    "        \n",
    "        a=chi0\n",
    "        c=(s2*(chi1-chi0)+s1*(chi0-chi2))/(s1*s1*s2-s1*s2*s2)\n",
    "        b=(chi1-chi0-c*s1*s1)/s1\n",
    "        \n",
    "        s_opt=-b/(2.0*c)\n",
    "        \n",
    "        # Update.\n",
    "        x_opt=x+s_opt*h\n",
    "        chi_opt=f.f(x_opt[0,0],x_opt[1,0],function)\n",
    "        \n",
    "    # Full search along the line. ------------------------\n",
    "    if ls_method==\"full\":\n",
    "        \n",
    "        # Initialise.\n",
    "        chi_opt=f.f(x[0,0],x[1,0],function)\n",
    "        x_opt=x\n",
    "        s_opt=s\n",
    "        \n",
    "        # March through many trial step lengths.\n",
    "        x_test=x+s*h\n",
    "        chi_test=f.f(x_test[0,0],x_test[1,0],function)\n",
    "        if chi_test>=chi_opt:\n",
    "            print('minimum step length too long')\n",
    "            s_opt=0.0\n",
    "            x_opt=x\n",
    "        else:\n",
    "\n",
    "            # Iterate.\n",
    "            while chi_test<chi_opt:\n",
    "                chi_opt=chi_test\n",
    "                x_opt=x_test\n",
    "                s_opt+=s\n",
    "                x_test=x+s_opt*h\n",
    "                chi_test=f.f(x_test[0,0],x_test[1,0],function)\n",
    "            \n",
    "    # Return. --------------------------------------------\n",
    "    return x_opt, chi_opt, s_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Steepest descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialise coordinates and misfit. --------------------\n",
    "x=np.matrix([[x0],[y0]])\n",
    "chi_sd=np.zeros(nit+1)\n",
    "chi_sd[0]=f.f(x[0,0],x[1,0],function)\n",
    "diff_sd=np.zeros(nit+1)\n",
    "diff_sd[0]=np.linalg.norm(x-opt)\n",
    "plt.subplots(1, figsize=(25,20))\n",
    "plt.plot(x[0,0],x[1,0],'k*',markersize=70)\n",
    "if print_misfit: print('iteration=0, misfit=%f' % chi_sd[0])\n",
    "\n",
    "s=s_min\n",
    "    \n",
    "# Iterate. ----------------------------------------------\n",
    "for it in range(nit):\n",
    "    \n",
    "    J=f.J(x[0,0],x[1,0],function)\n",
    "    x_new, chi_sd[it+1], s_opt = line_search(x,-J,s,function,ls_method)\n",
    "    if s_opt==0.0: s=s/2.0\n",
    "    plt.plot(x_new[0,0],x_new[1,0],'ko',markersize=20)\n",
    "    plt.plot((x[0,0],x_new[0,0]),(x[1,0],x_new[1,0]),'k',linewidth=4)\n",
    "    x=x_new\n",
    "    diff_sd[it+1]=np.linalg.norm(x-opt)\n",
    "        \n",
    "    if print_misfit: print('iteration=%d, misfit=%f' % (it+1, chi_sd[it+1]))\n",
    "\n",
    "# Plot trajectory. --------------------------------------\n",
    "f.f(xp,yp,function,plot=True)\n",
    "plt.tight_layout()\n",
    "if save_figure==True: plt.savefig(\"path_sd.pdf\",format='pdf')\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(1, figsize=(30,15))\n",
    "plt.semilogy(np.arange(0,nit+1),chi_sd,'k',linewidth=4)\n",
    "plt.xticks(np.arange(0,nit+1,2))\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('misfit')\n",
    "plt.grid()\n",
    "if save_figure==True: plt.savefig(\"misfit_sd.pdf\",format='pdf')\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(1, figsize=(30,15))\n",
    "plt.semilogy(np.arange(0,nit+1),diff_sd,'k',linewidth=4)\n",
    "plt.xticks(range(0,nit+1,2))\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('distance to optimum')\n",
    "plt.grid()\n",
    "if save_figure==True: plt.savefig(\"distance_sd.pdf\",format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Newton's method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialise coordinates and misfit. --------------------\n",
    "x=np.matrix([[x0],[y0]])\n",
    "chi_ne=np.zeros(nit+1)\n",
    "chi_ne[0]=f.f(x[0,0],x[1,0],function)\n",
    "diff_ne=np.zeros(nit+1)\n",
    "diff_ne[0]=np.linalg.norm(x-opt)\n",
    "plt.subplots(1, figsize=(30,25))\n",
    "plt.plot(x[0,0],x[1,0],'k*',markersize=70)\n",
    "if print_misfit: print('iteration=0, misfit=%f' % chi_ne[0])\n",
    "\n",
    "# Iterate. ----------------------------------------------\n",
    "for it in range(nit):\n",
    "    \n",
    "    J=f.J(x[0,0],x[1,0],function)\n",
    "    H=f.H(x[0,0],x[1,0],function)\n",
    "    h=-np.linalg.inv(H)*J\n",
    "    x_new=x+h\n",
    "    chi_ne[it+1]=f.f(x_new[0,0],x_new[1,0],function)\n",
    "    plt.plot(x_new[0,0],x_new[1,0],'ko',markersize=20)\n",
    "    plt.plot((x[0,0],x_new[0,0]),(x[1,0],x_new[1,0]),'k',linewidth=4)\n",
    "    x=x_new\n",
    "    diff_ne[it+1]=np.linalg.norm(x-opt)\n",
    "        \n",
    "    if print_misfit: print('iteration=%d, misfit=%f' % (it+1, chi_ne[it+1]))\n",
    "\n",
    "# Plot trajectory. --------------------------------------\n",
    "f.f(xp,yp,function,plot=True)\n",
    "plt.tight_layout()\n",
    "if save_figure==True: plt.savefig(\"path_newton.pdf\",format='pdf')\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(1, figsize=(30,15))\n",
    "plt.semilogy(np.arange(0,nit+1),chi_ne,'k',linewidth=4)\n",
    "plt.xticks(np.arange(0,nit+1,2))\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('misfit')\n",
    "plt.grid()\n",
    "if save_figure==True: plt.savefig(\"misfit_newton.pdf\",format='pdf')\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(1, figsize=(30,15))\n",
    "plt.semilogy(np.arange(0,nit+1),diff_ne,'k',linewidth=4)\n",
    "plt.xticks(np.arange(0,nit+1,2))\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('distance to optimum')\n",
    "plt.grid()\n",
    "if save_figure==True: plt.savefig(\"distance_newton.pdf\",format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conjugate gradients (Fletcher-Reeves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialise coordinates and misfit. --------------------\n",
    "x=np.matrix([[x0],[y0]])\n",
    "chi_fr=np.zeros(nit+1)\n",
    "chi_fr[0]=f.f(x[0,0],x[1,0],function)\n",
    "diff_fr=np.zeros(nit+1)\n",
    "diff_fr[0]=np.linalg.norm(x-opt)\n",
    "plt.subplots(1, figsize=(30,25))\n",
    "plt.plot(x[0,0],x[1,0],'k*',markersize=70)\n",
    "if print_misfit: print('iteration=0, misfit=%f' % chi_fr[0])\n",
    "\n",
    "s=s_min\n",
    "    \n",
    "# Iterate. ----------------------------------------------\n",
    "for it in range(nit):\n",
    "    \n",
    "    if it==0:\n",
    "        J=f.J(x[0,0],x[1,0],function)\n",
    "        h=-J\n",
    "        \n",
    "    x_new, chi_fr[it+1], s_opt = line_search(x,h,s,function,ls_method)\n",
    "    if s_opt==0.0: s=s/2.0\n",
    "    J_new=f.J(x_new[0,0],x_new[1,0],function)\n",
    "    beta=float((J_new.T*J_new)/(J.T*J))\n",
    "    h=-J_new+beta*h\n",
    "    \n",
    "    plt.plot(x_new[0,0],x_new[1,0],'ko',markersize=20)\n",
    "    plt.plot((x[0,0],x_new[0,0]),(x[1,0],x_new[1,0]),'k',linewidth=4)\n",
    "    \n",
    "    J=J_new\n",
    "    x=x_new\n",
    "    \n",
    "    diff_fr[it+1]=np.linalg.norm(x-opt)\n",
    "        \n",
    "    if print_misfit: print('iteration=%d, misfit=%f' % (it+1, chi_fr[it+1]))\n",
    "\n",
    "# Plot trajectory. --------------------------------------\n",
    "f.f(xp,yp,function,plot=True)\n",
    "plt.tight_layout()\n",
    "if save_figure==True: plt.savefig(\"path_cgfr.pdf\",format='pdf')\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(1, figsize=(30,15))\n",
    "plt.semilogy(np.arange(0,nit+1),chi_fr,'k',linewidth=4)\n",
    "plt.xticks(np.arange(0,nit+1,2))\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('misfit')\n",
    "plt.grid()\n",
    "if save_figure==True: plt.savefig(\"misfit_cgfr.pdf\",format='pdf')\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(1, figsize=(30,15))\n",
    "plt.semilogy(np.arange(0,nit+1),diff_fr,'k',linewidth=4)\n",
    "plt.xticks(np.arange(0,nit+1,2))\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('distance to optimum')\n",
    "plt.grid()\n",
    "if save_figure==True: plt.savefig(\"distance_cgfr.pdf\",format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conjugate gradients (Polak-Ribiere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise coordinates and misfit. --------------------\n",
    "x=np.matrix([[x0],[y0]])\n",
    "chi_pr=np.zeros(nit+1)\n",
    "chi_pr[0]=f.f(x[0,0],x[1,0],function)\n",
    "diff_pr=np.zeros(nit+1)\n",
    "diff_pr[0]=np.linalg.norm(x-opt)\n",
    "plt.subplots(1, figsize=(30,25))\n",
    "plt.plot(x[0,0],x[1,0],'k*',markersize=70)\n",
    "if print_misfit: print('iteration=0, misfit=%f' % chi_pr[0])\n",
    "\n",
    "s=s_min\n",
    "    \n",
    "# Iterate. ----------------------------------------------\n",
    "for it in range(nit):\n",
    "    \n",
    "    if it==0:\n",
    "        J=f.J(x[0,0],x[1,0],function)\n",
    "        h=-J\n",
    "        \n",
    "    x_new, chi_pr[it+1], s_opt = line_search(x,h,s,function,ls_method)\n",
    "    if s_opt==0.0: s=s/2.0\n",
    "    J_new=f.J(x_new[0,0],x_new[1,0],function)\n",
    "    beta=float((J_new.T*(J_new-J))/(J.T*J))\n",
    "    h=-J_new+beta*h\n",
    "    \n",
    "    plt.plot(x_new[0,0],x_new[1,0],'ko',markersize=20)\n",
    "    plt.plot((x[0,0],x_new[0,0]),(x[1,0],x_new[1,0]),'k',linewidth=4)\n",
    "    \n",
    "    J=J_new\n",
    "    x=x_new\n",
    "    \n",
    "    diff_pr[it+1]=np.linalg.norm(x-opt)\n",
    "        \n",
    "    if print_misfit: print('iteration=%d, misfit=%f' % (it+1, chi_pr[it+1]))\n",
    "\n",
    "# Plot trajectory. --------------------------------------\n",
    "f.f(xp,yp,function,plot=True)\n",
    "plt.tight_layout()\n",
    "if save_figure==True: plt.savefig(\"path_cgpr.pdf\",format='pdf')\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(1, figsize=(30,15))\n",
    "plt.semilogy(np.arange(0,nit+1),chi_pr,'k',linewidth=4)\n",
    "plt.xticks(np.arange(0,nit+1,2))\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('misfit')\n",
    "plt.grid()\n",
    "if save_figure==True: plt.savefig(\"misfit_cgpr.pdf\",format='pdf')\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(1, figsize=(30,15))\n",
    "plt.semilogy(np.arange(0,nit+1),diff_pr,'k',linewidth=4)\n",
    "plt.xticks(np.arange(0,nit+1,2))\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('distance to optimum')\n",
    "plt.grid()\n",
    "if save_figure==True: plt.savefig(\"distance_cgpr.pdf\",format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conjugate gradients (Hestenes-Stiefel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialise coordinates and misfit. --------------------\n",
    "x=np.matrix([[x0],[y0]])\n",
    "chi_hs=np.zeros(nit+1)\n",
    "chi_hs[0]=f.f(x[0,0],x[1,0],function)\n",
    "diff_hs=np.zeros(nit+1)\n",
    "diff_hs[0]=np.linalg.norm(x-opt)\n",
    "plt.subplots(1, figsize=(30,25))\n",
    "plt.plot(x[0,0],x[1,0],'k*',markersize=70)\n",
    "if print_misfit: print('iteration=0, misfit=%f' % chi[0])\n",
    "\n",
    "s=s_min\n",
    "    \n",
    "# Iterate. ----------------------------------------------\n",
    "for it in range(nit):\n",
    "    \n",
    "    if it==0:\n",
    "        J=f.J(x[0,0],x[1,0],function)\n",
    "        h=-J\n",
    "        \n",
    "    x_new, chi_hs[it+1], s_opt = line_search(x,h,s,function,ls_method)\n",
    "    if s_opt==0.0: s=s/2.0\n",
    "    J_new=f.J(x_new[0,0],x_new[1,0],function)\n",
    "    beta=float((J_new.T*(J_new-J))/(h.T*(J_new-J)))\n",
    "    h=-J_new+beta*h\n",
    "    \n",
    "    plt.plot(x_new[0,0],x_new[1,0],'ko',markersize=20)\n",
    "    plt.plot((x[0,0],x_new[0,0]),(x[1,0],x_new[1,0]),'k',linewidth=4)\n",
    "    \n",
    "    J=J_new\n",
    "    x=x_new\n",
    "    \n",
    "    diff_hs[it+1]=np.linalg.norm(x-opt)\n",
    "        \n",
    "    if print_misfit: print('iteration=%d, misfit=%f' % (it+1, chi_hs[it+1]))\n",
    "\n",
    "# Plot trajectory. --------------------------------------\n",
    "f.f(xp,yp,function,plot=True)\n",
    "plt.tight_layout()\n",
    "if save_figure==True: plt.savefig(\"path_cghs.pdf\",format='pdf')\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(1, figsize=(30,15))\n",
    "plt.semilogy(np.arange(0,nit+1),chi_hs,'k',linewidth=4)\n",
    "plt.xticks(np.arange(0,nit+1,2))\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('misfit')\n",
    "plt.grid()\n",
    "if save_figure==True: plt.savefig(\"misfit_cghs.pdf\",format='pdf')\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(1, figsize=(30,15))\n",
    "plt.semilogy(np.arange(0,nit+1),diff_hs,'k',linewidth=4)\n",
    "plt.xticks(np.arange(0,nit+1,2))\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('distance to optimum')\n",
    "plt.grid()\n",
    "if save_figure==True: plt.savefig(\"distance_cghs.pdf\",format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialise coordinates and misfit. --------------------\n",
    "x=np.matrix([[x0],[y0]])\n",
    "A=np.identity(2)\n",
    "I=np.identity(2)\n",
    "chi_bfgs=np.zeros(nit+1)\n",
    "chi_bfgs[0]=f.f(x[0,0],x[1,0],function)\n",
    "diff_bfgs=np.zeros(nit+1)\n",
    "diff_bfgs[0]=np.linalg.norm(x-opt)\n",
    "plt.subplots(1, figsize=(30,25))\n",
    "plt.plot(x[0,0],x[1,0],'k*',markersize=70)\n",
    "if print_misfit: print('iteration=0, misfit=%f' % chi[0])\n",
    "\n",
    "s=s_min\n",
    "    \n",
    "# Iterate. ----------------------------------------------\n",
    "for it in range(nit):\n",
    "    \n",
    "    # Compute descent direction.\n",
    "    J=f.J(x[0,0],x[1,0],function)\n",
    "    h=-A*J\n",
    "        \n",
    "    # Compute update of position and of gradient\n",
    "    x_new, chi_bfgs[it+1], s_opt = line_search(x,h,s,function,ls_method)\n",
    "    if s_opt==0.0: s=s/2.0\n",
    "    J_new=f.J(x_new[0,0],x_new[1,0],function)\n",
    "    \n",
    "    # Compute position and gradient differences.\n",
    "    b=x_new-x\n",
    "    y=J_new-J\n",
    "    \n",
    "    # Update the matrix A.\n",
    "    rho=float(1.0/(y.T*b))\n",
    "    A=(I-rho*b*y.T)*A*(I-rho*y*b.T)+rho*b*b.T\n",
    "    \n",
    "    # Plot next step of the trajectory.\n",
    "    plt.plot(x_new[0,0],x_new[1,0],'ko',markersize=20)\n",
    "    plt.plot((x[0,0],x_new[0,0]),(x[1,0],x_new[1,0]),'k',linewidth=4)\n",
    "    \n",
    "    # Move position and gradient to the next iteration.\n",
    "    J=J_new\n",
    "    x=x_new\n",
    "    \n",
    "    diff_bfgs[it+1]=np.linalg.norm(x-opt)\n",
    "        \n",
    "    if print_misfit: print('iteration=%d, misfit=%f' % (it+1, chi_bfgs[it+1]))\n",
    "\n",
    "# Plot trajectory. --------------------------------------\n",
    "f.f(xp,yp,function,plot=True)\n",
    "plt.tight_layout()\n",
    "if save_figure==True: plt.savefig(\"path_bfgs.pdf\",format='pdf')\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(1, figsize=(30,15))\n",
    "plt.semilogy(np.arange(0,nit+1),chi_bfgs,'k',linewidth=4)\n",
    "plt.xticks(np.arange(0,nit+1,2))\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('misfit')\n",
    "plt.grid()\n",
    "if save_figure==True: plt.savefig(\"misfit_bfgs.pdf\",format='pdf')\n",
    "plt.show()\n",
    "\n",
    "np.save('chi_bfgs',chi_bfgs)\n",
    "\n",
    "plt.subplots(1, figsize=(30,15))\n",
    "plt.semilogy(np.arange(0,nit+1),diff_bfgs,'k',linewidth=4)\n",
    "plt.xticks(np.arange(0,nit+1,2))\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('distance to optimum')\n",
    "plt.grid()\n",
    "if save_figure==True: plt.savefig(\"distance_bfgs.pdf\",format='pdf')\n",
    "plt.show()\n",
    "\n",
    "# Print correct and approximate inverse Hessians.\n",
    "print('exact inverse Hessian in the final iteration:')\n",
    "print(f.H(x[0,0],x[1,0],function))\n",
    "print('approximate inverse Hessian in the final iteration')\n",
    "print(np.linalg.inv(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. L-BFGS\n",
    "\n",
    "The L-BFGS method requires a little bit more algorithmic overhead. Before going to the iteration itself, we provide a Python class that stores the $k$ most recent vector pairs and performs the internal L-BFGS iterations to compute the negative descent direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lbfgs:\n",
    "    \n",
    "    def __init__(self,k):\n",
    "        \"\"\"\n",
    "        Initialise the BFGS class with zero vectors.\n",
    "        \n",
    "        :param k: Maximum number of vectors.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.k=k # Maximum number of vectors.\n",
    "        self.i=0 # Currently stored vectors.\n",
    "        self.s=np.zeros((2,k)) # s vectors.\n",
    "        self.y=np.zeros((2,k)) # y vectors.\n",
    "        \n",
    "    def put(self,s,y):\n",
    "        \"\"\"\n",
    "        Update the stored vector pairs.\n",
    "        \n",
    "        :param s: New s vector.\n",
    "        :param y: New y vector.\n",
    "        \"\"\"\n",
    "        \n",
    "        # When less then k vector pairs are stored, we increase the index of stored pairs and add the new pair.\n",
    "        if self.i<self.k:\n",
    "            self.i+=1\n",
    "        # Otherwise we kick out the pair with the lowest index by rolling to the left and over-writing the last pair.\n",
    "        else:\n",
    "            self.s=np.roll(self.s, -1, axis=1)\n",
    "            self.y=np.roll(self.y, -1, axis=1)\n",
    "        \n",
    "        self.s[:,self.i-1]=s\n",
    "        self.y[:,self.i-1]=y\n",
    "        \n",
    "    def iterate(self,q):\n",
    "        \"\"\"\n",
    "        Perform the 2 internal L-BFGS iterations.\n",
    "        \n",
    "        :param q: Current gradient vector.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Iteration 1. -------------------------------------\n",
    "        alpha=np.zeros(self.i)\n",
    "        A0=np.identity(2)\n",
    "        \n",
    "        for n in range(self.i-1,-1,-1):\n",
    "            rho=1.0/np.dot(self.y[:,n],self.s[:,n])\n",
    "            alpha[n]=rho*np.dot(self.s[:,n],q)\n",
    "            q=q-alpha[n]*self.y[:,n]\n",
    "       \n",
    "        r=np.dot(A0,q)\n",
    "        \n",
    "        # Iteration 2. -------------------------------------\n",
    "        for n in range(0,self.i):\n",
    "            rho=1.0/np.dot(self.y[:,n],self.s[:,n])\n",
    "            beta=rho*np.dot(self.y[:,n],r)\n",
    "            r=r+(alpha[n]-beta)*self.s[:,n]\n",
    "        \n",
    "        # Return the negative descent direction.\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialise coordinates and misfit. --------------------\n",
    "x=np.matrix([[x0],[y0]])\n",
    "chi_lbfgs=np.zeros(nit+1)\n",
    "chi_lbfgs[0]=f.f(x[0,0],x[1,0],function)\n",
    "diff_lbfgs=np.zeros(nit+1)\n",
    "diff_lbfgs[0]=np.linalg.norm(x-opt)\n",
    "plt.subplots(1, figsize=(30,25))\n",
    "plt.plot(x[0,0],x[1,0],'k*',markersize=70)\n",
    "if print_misfit: print('iteration=0, misfit=%f' % chi_lbfgs[0])\n",
    "\n",
    "L=lbfgs(5)\n",
    "\n",
    "s=s_min\n",
    "\n",
    "# Iterate. ----------------------------------------------\n",
    "for it in range(nit):\n",
    "    \n",
    "    # Compute descent direction.\n",
    "    J=f.J(x[0,0],x[1,0],function)\n",
    "    if it==0:\n",
    "        h=-J\n",
    "    else:\n",
    "        h=-L.iterate(np.ravel(J))\n",
    "        h=np.reshape(h,(2,1))\n",
    "       \n",
    "    # Compute update of position and of gradient\n",
    "    x_new, chi_lbfgs[it+1], s_opt = line_search(x,h,s,function,ls_method)\n",
    "    if s_opt==0.0: s=s/2.0\n",
    "    J_new=f.J(x_new[0,0],x_new[1,0],function)\n",
    "    \n",
    "    # Compute position and gradient differences.\n",
    "    b=x_new-x\n",
    "    y=J_new-J\n",
    "    \n",
    "    # Update the set of vector pairs.\n",
    "    L.put(np.ravel(b),np.ravel(y))\n",
    "    \n",
    "    # Plot next step of the trajectory.\n",
    "    plt.plot(x_new[0,0],x_new[1,0],'ko',markersize=20)\n",
    "    plt.plot((x[0,0],x_new[0,0]),(x[1,0],x_new[1,0]),'k',linewidth=4)\n",
    "    \n",
    "    # Move position and gradient to the next iteration.\n",
    "    J=J_new\n",
    "    x=x_new\n",
    "    \n",
    "    diff_lbfgs[it+1]=np.linalg.norm(x-opt)\n",
    "        \n",
    "    if print_misfit: print('iteration=%d, misfit=%f' % (it+1, chi_lbfgs[it+1]))\n",
    "\n",
    "# Plot trajectory. --------------------------------------\n",
    "f.f(xp,yp,function,plot=True)\n",
    "plt.tight_layout()\n",
    "if save_figure==True: plt.savefig(\"path_lbfgs.pdf\",format='pdf')\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(1, figsize=(30,15))\n",
    "plt.semilogy(np.arange(0,nit+1),chi_lbfgs,'k',linewidth=4)\n",
    "plt.xticks(np.arange(0,nit+1,2))\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('misfit')\n",
    "plt.grid()\n",
    "if save_figure==True: plt.savefig(\"misfit_lbfgs.pdf\",format='pdf')\n",
    "plt.show()\n",
    "\n",
    "plt.subplots(1, figsize=(30,15))\n",
    "plt.semilogy(np.arange(0,nit+1),diff_lbfgs,'k',linewidth=4)\n",
    "plt.xticks(np.arange(0,nit+1,2))\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('distance to optimum')\n",
    "plt.grid()\n",
    "if save_figure==True: plt.savefig(\"distance_lbfgs.pdf\",format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Convergence summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misfit\n",
    "plt.subplots(1, figsize=(30,15))\n",
    "plt.semilogy(np.arange(0,nit+1),chi_lbfgs,'y',label='L-BFGS',linewidth=4)\n",
    "plt.semilogy(np.arange(0,nit+1),chi_bfgs,'b',label='BFGS',linewidth=4)\n",
    "plt.semilogy(np.arange(0,nit+1),chi_ne,'r',label='Newton',linewidth=4)\n",
    "plt.semilogy(np.arange(0,nit+1),chi_fr,'g',label='CG-FR',linewidth=4)\n",
    "plt.semilogy(np.arange(0,nit+1),chi_sd,'k',label='Steepest-Descent',linewidth=4)\n",
    "\n",
    "#plt.legend()\n",
    "plt.xticks(np.arange(0,nit+1))\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('misfit')\n",
    "plt.grid()\n",
    "plt.xlim([0,nit])\n",
    "plt.ylim([1e-15,500.0])\n",
    "plt.xticks(range(0,nit,3))\n",
    "plt.tight_layout()\n",
    "if save_figure==True: plt.savefig(\"comparison_misfits.pdf\",format='pdf')\n",
    "plt.show()\n",
    "\n",
    "#Distance to optimum\n",
    "plt.subplots(1, figsize=(30,15))\n",
    "plt.semilogy(np.arange(0,nit+1),diff_lbfgs,'y',label='L-BFGS',linewidth=4)\n",
    "plt.semilogy(np.arange(0,nit+1),diff_bfgs,'b',label='BFGS',linewidth=4)\n",
    "plt.semilogy(np.arange(0,nit+1),diff_ne,'r',label='Newton',linewidth=4)\n",
    "plt.semilogy(np.arange(0,nit+1),diff_fr,'g',label='CG-FR',linewidth=4)\n",
    "plt.semilogy(np.arange(0,nit+1),diff_sd,'k',label='Steepest-Descent',linewidth=4)\n",
    "\n",
    "#plt.legend()\n",
    "plt.xticks(np.arange(0,nit+1))\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('distance to optimum')\n",
    "plt.grid()\n",
    "plt.xlim([0,nit])\n",
    "plt.ylim([1e-10,10.0])\n",
    "plt.xticks(range(0,nit,3))\n",
    "plt.tight_layout()\n",
    "if save_figure==True: plt.savefig(\"comparison_distance.pdf\",format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Exercises\n",
    "\n",
    "**Exercise 1**: Visualise the quadratic, Rosenbrock, Himmelblau and Bazaraa-Shetty test functions and discuss their properties (location and number of minima, saddle points, etc.).\n",
    "\n",
    "**Exercise 2**: Modify the initial parameter values (initial coordinates) for the steepest-descent optimisation of the Rosenbrack function. Can you find initial values at a distance of more than $\\sim$0.25 from the optimum, from where the steepest descent algorithm converges in just one step (maybe not exactly but almost)?\n",
    "\n",
    "**Exercise 3**: Explain why Newton's method (applied to the Rosenbrock function) may in some steps actually move upwards?\n",
    "\n",
    "**Exercise 4**: Experiment with the number of iterations that the steepest descent algorithm needs to achieve a distance to the optimum that is similar to the results after 10 iterations for the other optimisation algorithms.\n",
    "\n",
    "**Exercise 5**: Numerically investigate the behaviour of Newton's method and the quasi-Newton methods (BFGS, L-BFGS) when the initial position is near one of the saddle points of the Himmelblau function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
